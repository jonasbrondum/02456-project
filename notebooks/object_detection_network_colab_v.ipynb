{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "object_detection_network_colab_v.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fd3feabac9b74cf18524722275a3942a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3970e671338b4f4a899828405da06d8a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_482ead2e756b436698212e4b95f121e8",
              "IPY_MODEL_5f99df2a91f14ac7acefde920d1beefe",
              "IPY_MODEL_94991210020c4c129bcf1603991700d9"
            ]
          }
        },
        "3970e671338b4f4a899828405da06d8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "482ead2e756b436698212e4b95f121e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7d65e1cf6af04b158c02bea8456fb172",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d6b24588f27a49df8cdadb44220b0784"
          }
        },
        "5f99df2a91f14ac7acefde920d1beefe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_634cea8627854a7aafb0f4f35347b9ea",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 167502836,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 167502836,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_44bcd3757582444c8e4f3c252ba53401"
          }
        },
        "94991210020c4c129bcf1603991700d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_006bad4ce139474fb4bf548aaaa18f2c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 160M/160M [00:02&lt;00:00, 81.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_56bacfe747b6411e82596ccb1229b943"
          }
        },
        "7d65e1cf6af04b158c02bea8456fb172": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d6b24588f27a49df8cdadb44220b0784": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "634cea8627854a7aafb0f4f35347b9ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "44bcd3757582444c8e4f3c252ba53401": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "006bad4ce139474fb4bf548aaaa18f2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "56bacfe747b6411e82596ccb1229b943": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqRJWcHrpisf"
      },
      "source": [
        "# Training and testing of objection detection network\n",
        "Based on Faster-RCNN and this guide: https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbLnf9KRpish"
      },
      "source": [
        "# %%shell\n",
        "#pip install cython\n",
        "# # Install pycocotools, the version by default in Colab\n",
        "# # has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354\n",
        "#pip install pycocotools -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDz3Rf-wpisj"
      },
      "source": [
        "## Importing libraries and custom scripts\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1F-4lhTOiVh_"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "\n",
        "# Cloning git repo\n",
        "\n",
        "!git clone https://github.com/jonasbrondum/02456-project.git\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cZHnjDEjRQY"
      },
      "source": [
        "import os\n",
        "os.getcwd()\n",
        "sys.path.append(os.getcwd() + \"/02456-project/scripts_colab\")\n",
        "sys.path.append(os.getcwd() + \"/02456-project\")\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ib_mzhv7kZ__",
        "outputId": "a1a74877-47e6-4310-aeaf-8fdf04cd0f32"
      },
      "source": [
        "print(sys.path)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '/content', '/env/python', '/usr/lib/python37.zip', '/usr/lib/python3.7', '/usr/lib/python3.7/lib-dynload', '/usr/local/lib/python3.7/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.7/dist-packages/IPython/extensions', '/root/.ipython', '/content/02456-project/scripts_colab', '/content/02456-project/scripts_colab', '/content/02456-project']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgHTFCRjpisk"
      },
      "source": [
        "## Dataset class with new __getitem__ function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAduM2anpisk"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from PIL import Image\n",
        "import scripts_colab.transforms as T\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class CansDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, train=True):\n",
        "        if train is True:\n",
        "            self.root = root + \"/video1/train/\"\n",
        "        else:\n",
        "            self.root = root +  \"/video1/test/\"\n",
        "\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(self.root,\"frames\"))))\n",
        "        self.bbox = list(sorted(os.listdir(os.path.join(self.root,\"boundingboxes\"))))\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # load images and bboxes\n",
        "        img_path = os.path.join(self.root, \"frames\", self.imgs[idx])\n",
        "        bbox_path = os.path.join(self.root, \"boundingboxes\", self.bbox[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img=np.array(img)\n",
        "        img=torch.tensor(img)/255\n",
        "        img=img.permute(2,0,1)\n",
        "\n",
        "        bbox = []\n",
        "        label = []\n",
        "        lines = 0\n",
        "        iscrowd = []\n",
        "        with open(bbox_path, 'r') as f:\n",
        "            for line in f:\n",
        "                line = line.split(\" \")\n",
        "                id = line[0] # class label, 1=beer, 2=cola, 0=background\n",
        "                id = 1 if id == 'beer' else 2\n",
        "                xmin = float(line[1])\n",
        "                ymin = float(line[2])\n",
        "                xmax = float(line[3])\n",
        "                ymax = float(line[4])\n",
        "                bbox.append([xmin, ymin, xmax, ymax])\n",
        "                label.append(id)\n",
        "                lines += 1\n",
        "                iscrowd.append(False)\n",
        "\n",
        "        bbox = torch.as_tensor(bbox, dtype=torch.int64)\n",
        "        labels = torch.as_tensor(label, dtype=torch.int64) #torch.ones((num_objs, ), dtype=torch.float32)\n",
        "        image_id = torch.tensor([idx],dtype=torch.int64)\n",
        "        area = (bbox[:, 3] - bbox[:, 1]) * (bbox[:, 2] - bbox[:, 0])\n",
        "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
        "        #iscrowd = torch.zeros((lines,), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = bbox\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        # if no boxes\n",
        "        # torch.zeros((0,4), dtype=torch.float32)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9khnowypisl"
      },
      "source": [
        "Test class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33Z9xU7Upism"
      },
      "source": [
        "\n",
        "#root = os.getcwd()+'data/'#os.getcwd() + '/..' + '/data/'\n",
        "#sys.path.append(os.getcwd() + \"/..\" + \"/scripts_colab\") \n",
        "#sys.path.append(os.getcwd() + \"/..\") \n",
        "#print(os.getcwd())\n",
        "#os.chdir(os.getcwd()+\"/..\")\n",
        "\n",
        "#print(root)\n",
        "root = os.getcwd() + \"/02456-project/data\""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDyzV8cHpism"
      },
      "source": [
        "## Adding pretrained model and modify numbers of classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYcszTqFpisn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "fd3feabac9b74cf18524722275a3942a",
            "3970e671338b4f4a899828405da06d8a",
            "482ead2e756b436698212e4b95f121e8",
            "5f99df2a91f14ac7acefde920d1beefe",
            "94991210020c4c129bcf1603991700d9",
            "7d65e1cf6af04b158c02bea8456fb172",
            "d6b24588f27a49df8cdadb44220b0784",
            "634cea8627854a7aafb0f4f35347b9ea",
            "44bcd3757582444c8e4f3c252ba53401",
            "006bad4ce139474fb4bf548aaaa18f2c",
            "56bacfe747b6411e82596ccb1229b943"
          ]
        },
        "outputId": "058df94a-3a05-4641-c87f-9cc92c04b35c"
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "# load a model pre-trained pre-trained on COCO\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "# replace the classifier with a new one, that has\n",
        "# num_classes which is user-defined\n",
        "num_classes = 3  # 1 class (person) + background\n",
        "# get number of input features for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "# replace the pre-trained head with a new one\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd3feabac9b74cf18524722275a3942a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/160M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Viv29kHjpisn"
      },
      "source": [
        "def get_instance_segmentation_model(num_classes):\n",
        "    # load an instance segmentation model pre-trained on COCO\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    # get the number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    # now get the number of input features for the mask classifier\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "    # and replace the mask predictor with a new one\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
        "                                                       hidden_layer,\n",
        "                                                       num_classes)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQr3wL5apisn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8bba792-13d2-4741-fcff-68708abfdb1d"
      },
      "source": [
        "from scripts_colab.engine import train_one_epoch, evaluate\n",
        "\n",
        "import scripts_colab.utils\n",
        "import scripts_colab.transforms as T\n",
        "from typing import List, Tuple\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "dataset = CansDataset(root, train=True)\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=2, shuffle=True, num_workers=2,\n",
        "    collate_fn=scripts_colab.utils.collate_fn\n",
        ")\n",
        "# For Training\n",
        "images,targets = next(iter(data_loader))\n",
        "images = list(image for image in images)\n",
        "\n",
        "\n",
        "print(images[0])\n",
        "\n",
        "targets = [{k: v for k, v in t.items()} for t in targets]\n",
        "\n",
        "output = model(images,targets)   # Returns losses and detections\n",
        "# For inference\n",
        "model.eval()\n",
        "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
        "predictions = model(x)           # Returns predictions\n",
        "print(predictions)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.3765, 0.3765, 0.3961,  ..., 0.3529, 0.3647, 0.3569],\n",
            "         [0.3961, 0.3490, 0.3176,  ..., 0.3569, 0.3725, 0.3647],\n",
            "         [0.3882, 0.3255, 0.3059,  ..., 0.3529, 0.3686, 0.3608],\n",
            "         ...,\n",
            "         [0.1020, 0.1020, 0.1020,  ..., 0.0706, 0.0706, 0.0706],\n",
            "         [0.0980, 0.1020, 0.1020,  ..., 0.0745, 0.0745, 0.0745],\n",
            "         [0.0980, 0.0980, 0.0980,  ..., 0.0745, 0.0745, 0.0745]],\n",
            "\n",
            "        [[0.3255, 0.3255, 0.3373,  ..., 0.3020, 0.3137, 0.3098],\n",
            "         [0.3373, 0.2902, 0.2627,  ..., 0.3059, 0.3216, 0.3176],\n",
            "         [0.3294, 0.2667, 0.2510,  ..., 0.3020, 0.3176, 0.3137],\n",
            "         ...,\n",
            "         [0.1059, 0.1059, 0.1059,  ..., 0.0745, 0.0745, 0.0745],\n",
            "         [0.1020, 0.1059, 0.1059,  ..., 0.0784, 0.0784, 0.0784],\n",
            "         [0.1020, 0.1020, 0.1020,  ..., 0.0784, 0.0784, 0.0784]],\n",
            "\n",
            "        [[0.2588, 0.2510, 0.2471,  ..., 0.1686, 0.1765, 0.1608],\n",
            "         [0.2627, 0.2078, 0.1608,  ..., 0.1725, 0.1843, 0.1686],\n",
            "         [0.2471, 0.1765, 0.1490,  ..., 0.1765, 0.1804, 0.1647],\n",
            "         ...,\n",
            "         [0.0824, 0.0824, 0.0824,  ..., 0.0549, 0.0549, 0.0549],\n",
            "         [0.0784, 0.0824, 0.0824,  ..., 0.0588, 0.0588, 0.0588],\n",
            "         [0.0784, 0.0784, 0.0784,  ..., 0.0588, 0.0588, 0.0588]]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'boxes': tensor([[  0.0000,  12.8552, 308.3311, 178.9292],\n",
            "        [ 40.1656, 188.9036,  46.7233, 194.9113],\n",
            "        [  0.0000,  35.4238, 184.1772, 188.6429],\n",
            "        [342.6573,  12.8271, 400.0000, 195.7937],\n",
            "        [  0.0000, 168.2077, 239.9594, 248.0885],\n",
            "        [ 88.5754,  77.3201,  93.0437,  80.6575],\n",
            "        [ 34.7917, 142.0704, 259.1571, 224.5171],\n",
            "        [  0.0000,  59.8464, 348.2917, 235.1200],\n",
            "        [297.9188,  61.5508, 302.5425,  66.1730],\n",
            "        [ 30.3133, 107.6977, 263.2783, 184.9244],\n",
            "        [200.0288,  28.2272, 400.0000, 126.4453],\n",
            "        [150.5571,  15.5129, 383.5333, 163.4640],\n",
            "        [309.8131, 224.5099, 314.1648, 228.6493],\n",
            "        [ 92.0823, 151.8054, 325.4271, 239.1302],\n",
            "        [282.3657,  95.0826, 286.2843, 101.3401],\n",
            "        [  0.0000,  95.6831, 193.1662, 249.1653],\n",
            "        [ 41.0164, 191.1950,  47.2054, 197.1384],\n",
            "        [334.8720,  78.6036, 339.1789,  84.7026],\n",
            "        [298.9394, 227.0254, 305.0697, 231.4228],\n",
            "        [  9.6293,  15.1959, 101.3057, 194.8833],\n",
            "        [ 73.1974, 150.5199,  76.9399, 154.3924],\n",
            "        [295.3894,  95.9153, 299.4798, 101.0038],\n",
            "        [ 87.0181,  76.8847,  90.0349,  80.5617],\n",
            "        [109.9145, 132.7538, 114.1881, 136.9391],\n",
            "        [ 94.6203, 205.3645,  99.9835, 209.4247],\n",
            "        [ 74.9797, 150.1716,  78.7348, 154.2997],\n",
            "        [ 74.9607, 151.4675,  78.8315, 155.8814],\n",
            "        [301.8315,  99.6847, 306.2397, 104.0449],\n",
            "        [302.1020, 101.5443, 306.4953, 105.2431],\n",
            "        [110.8857, 134.4867, 114.9088, 139.1155],\n",
            "        [112.6097, 134.0135, 116.5777, 138.7208],\n",
            "        [ 89.9313,  75.9792,  93.4898,  80.0950],\n",
            "        [ 96.3561, 205.6338, 101.2134, 209.6515],\n",
            "        [ 83.0847,  59.0957, 304.0537, 137.5942],\n",
            "        [ 91.8528,  84.7940, 343.2362, 168.5669],\n",
            "        [ 74.7338, 148.7537,  78.4969, 152.6370],\n",
            "        [309.7365, 222.2898, 314.5366, 226.4404],\n",
            "        [ 74.5468, 208.6481, 400.0000, 293.7988],\n",
            "        [108.3180, 183.0958, 111.9439, 187.1225],\n",
            "        [145.3091, 121.7938, 397.0970, 219.2768],\n",
            "        [110.5059, 188.5500, 114.6344, 191.8718],\n",
            "        [269.5995,  64.2276, 273.9249,  67.4648],\n",
            "        [109.2367, 131.8554, 112.9636, 136.4508],\n",
            "        [ 70.9382, 149.2333,  74.2834, 153.0437],\n",
            "        [108.5222, 184.4924, 112.7026, 187.6770],\n",
            "        [296.7758, 219.9381, 300.7674, 224.8407],\n",
            "        [308.7714, 226.5473, 313.0126, 230.8308],\n",
            "        [ 71.8137, 148.8970,  75.5134, 152.6754],\n",
            "        [111.6699, 188.1700, 115.8880, 191.7700],\n",
            "        [  0.0000,  29.7340, 115.1247, 300.0000],\n",
            "        [109.2334, 134.9855, 113.0932, 139.4186],\n",
            "        [325.2001, 112.9425, 329.4633, 118.8077],\n",
            "        [300.1729, 101.2372, 305.7049, 104.4793],\n",
            "        [ 88.3591, 143.3050,  92.0635, 147.4143],\n",
            "        [297.8924,  63.8890, 302.5229,  68.7350],\n",
            "        [ 64.0194, 153.6563,  68.4448, 157.9619],\n",
            "        [ 73.0678, 148.6839,  76.8016, 152.5572],\n",
            "        [299.7936,  58.9314, 303.9600,  63.9874],\n",
            "        [297.1653,  60.8466, 301.4729,  65.0374],\n",
            "        [ 71.0817, 147.1957,  74.2224, 151.0056],\n",
            "        [336.3250,  77.9592, 340.4956,  84.5319],\n",
            "        [276.0562,  94.7721, 280.0890, 100.6265],\n",
            "        [172.8813, 155.8625, 176.9617, 161.6487],\n",
            "        [204.2484,  92.7232, 400.0000, 172.2640],\n",
            "        [113.5061, 132.9441, 117.5510, 137.8648],\n",
            "        [109.5534, 183.4566, 113.4409, 187.2678],\n",
            "        [289.3839,   0.0000, 400.0000, 282.3096],\n",
            "        [108.8352, 188.8396, 112.8931, 192.2623],\n",
            "        [342.0986, 191.0481, 345.5431, 195.7838],\n",
            "        [106.8605, 125.6404, 109.8928, 129.3172],\n",
            "        [ 62.3254, 153.9084,  66.6341, 158.1948],\n",
            "        [306.2180,  99.0509, 309.8758, 102.6522],\n",
            "        [209.9444, 190.9924, 213.6621, 195.3848],\n",
            "        [295.7446, 219.0643, 299.7221, 224.3335],\n",
            "        [ 59.8421, 155.3110,  63.8529, 159.7971],\n",
            "        [ 82.9472, 204.5966,  86.7312, 209.3024],\n",
            "        [ 73.3181, 147.2033,  77.0016, 151.0311],\n",
            "        [ 85.3561,  74.9825,  88.7365,  78.8801],\n",
            "        [110.7761, 183.1639, 114.5127, 186.8184],\n",
            "        [298.3733, 225.5314, 304.4309, 230.1400],\n",
            "        [269.2269,  60.1275, 273.2135,  63.1059],\n",
            "        [307.5312, 223.4571, 311.6399, 228.1656],\n",
            "        [299.8333,  62.1112, 303.9299,  67.4320],\n",
            "        [ 36.3898,   1.1941, 400.0000, 112.7533],\n",
            "        [113.9072, 135.3257, 117.5668, 140.3863],\n",
            "        [257.6173, 142.9841, 262.4320, 146.8675],\n",
            "        [270.1851,  57.9935, 273.3136,  60.8716],\n",
            "        [210.4636, 220.5100, 400.0000, 274.7430],\n",
            "        [ 67.0617, 152.7021,  70.9275, 156.9475],\n",
            "        [191.8096, 155.1806, 400.0000, 263.4058],\n",
            "        [107.7503, 132.1933, 111.5151, 136.6330],\n",
            "        [ 70.9896, 145.7411,  74.1634, 149.5203],\n",
            "        [ 64.0827, 152.3062,  68.4036, 156.4846],\n",
            "        [108.4003, 126.2433, 111.5298, 129.7257],\n",
            "        [317.0744, 104.6658, 320.6961, 108.8854],\n",
            "        [268.7551,  57.8575, 271.9626,  60.8048],\n",
            "        [ 74.7761, 146.9683,  78.5360, 150.6658],\n",
            "        [111.2462, 131.1320, 115.1310, 135.9114],\n",
            "        [108.3906, 124.6184, 111.5082, 127.9641],\n",
            "        [268.7582,  59.2022, 271.9911,  62.2022]], grad_fn=<StackBackward0>), 'labels': tensor([2, 1, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2,\n",
            "        1, 2, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2,\n",
            "        2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2]), 'scores': tensor([0.4631, 0.4280, 0.4196, 0.4117, 0.4113, 0.4082, 0.4068, 0.4063, 0.4059,\n",
            "        0.4010, 0.4005, 0.3986, 0.3955, 0.3944, 0.3939, 0.3937, 0.3935, 0.3932,\n",
            "        0.3932, 0.3920, 0.3918, 0.3915, 0.3915, 0.3897, 0.3896, 0.3892, 0.3890,\n",
            "        0.3890, 0.3888, 0.3886, 0.3886, 0.3872, 0.3866, 0.3861, 0.3858, 0.3854,\n",
            "        0.3852, 0.3851, 0.3848, 0.3844, 0.3844, 0.3844, 0.3840, 0.3839, 0.3839,\n",
            "        0.3835, 0.3835, 0.3830, 0.3823, 0.3819, 0.3815, 0.3810, 0.3809, 0.3809,\n",
            "        0.3801, 0.3799, 0.3799, 0.3796, 0.3795, 0.3792, 0.3790, 0.3789, 0.3789,\n",
            "        0.3786, 0.3785, 0.3784, 0.3782, 0.3775, 0.3771, 0.3768, 0.3767, 0.3766,\n",
            "        0.3765, 0.3765, 0.3763, 0.3761, 0.3761, 0.3758, 0.3756, 0.3753, 0.3752,\n",
            "        0.3748, 0.3748, 0.3744, 0.3741, 0.3738, 0.3738, 0.3738, 0.3737, 0.3736,\n",
            "        0.3731, 0.3729, 0.3729, 0.3728, 0.3727, 0.3727, 0.3726, 0.3721, 0.3720,\n",
            "        0.3719], grad_fn=<IndexBackward0>)}, {'boxes': tensor([[3.5041e+02, 1.5724e+02, 3.5734e+02, 1.6548e+02],\n",
            "        [2.4145e+02, 3.4310e+02, 4.0000e+02, 4.7834e+02],\n",
            "        [2.9743e+02, 2.4416e+02, 4.0000e+02, 4.8843e+02],\n",
            "        [3.5179e+02, 1.6016e+02, 3.5781e+02, 1.6710e+02],\n",
            "        [3.5025e+02, 1.6182e+02, 3.5758e+02, 1.6949e+02],\n",
            "        [2.7889e+02, 3.5986e+02, 2.8442e+02, 3.6422e+02],\n",
            "        [8.1266e+01, 4.5091e+02, 4.0000e+02, 4.9251e+02],\n",
            "        [0.0000e+00, 0.0000e+00, 2.7048e+02, 2.6542e+02],\n",
            "        [2.7744e+02, 3.6147e+02, 2.8302e+02, 3.6585e+02],\n",
            "        [2.7837e+02, 3.5843e+02, 2.8404e+02, 3.6222e+02],\n",
            "        [3.5205e+02, 8.7838e+01, 4.0000e+02, 3.3246e+02],\n",
            "        [3.3426e+02, 3.3994e+01, 4.0000e+02, 2.6778e+02],\n",
            "        [0.0000e+00, 4.6247e+01, 2.1994e+02, 4.1528e+02],\n",
            "        [0.0000e+00, 4.6083e+01, 2.5507e+02, 1.7990e+02],\n",
            "        [2.0925e+02, 2.1565e+02, 4.0000e+02, 4.8209e+02],\n",
            "        [2.7702e+02, 3.5925e+02, 2.8307e+02, 3.6288e+02],\n",
            "        [6.0162e+00, 2.4018e+01, 2.4626e+02, 1.1080e+02],\n",
            "        [1.1107e+02, 5.1687e+01, 4.0000e+02, 1.7858e+02],\n",
            "        [2.7599e+02, 8.9057e+01, 2.8143e+02, 9.3863e+01],\n",
            "        [1.2859e+02, 1.3591e+02, 1.3722e+02, 1.4011e+02],\n",
            "        [2.5760e+02, 3.7473e+02, 2.6435e+02, 3.7950e+02],\n",
            "        [1.4901e+02, 3.4365e+02, 1.5434e+02, 3.4939e+02],\n",
            "        [1.8978e+02, 1.5247e+02, 1.9514e+02, 1.5716e+02],\n",
            "        [2.7732e+02, 3.6484e+02, 2.8307e+02, 3.6896e+02],\n",
            "        [3.3476e+02, 1.5174e+02, 4.0000e+02, 3.9004e+02],\n",
            "        [1.8760e+02, 1.5204e+02, 1.9374e+02, 1.5684e+02],\n",
            "        [8.6878e+01, 1.2812e+02, 3.9959e+02, 2.3559e+02],\n",
            "        [2.7603e+02, 3.6055e+02, 2.8181e+02, 3.6414e+02],\n",
            "        [0.0000e+00, 3.8138e+02, 4.0000e+02, 4.9217e+02],\n",
            "        [1.7101e+02, 3.6010e+02, 1.7641e+02, 3.6481e+02],\n",
            "        [5.8502e+01, 3.9445e+01, 3.5740e+02, 2.4936e+02],\n",
            "        [1.9361e+02, 1.5558e+02, 1.9954e+02, 1.6003e+02],\n",
            "        [2.9947e+02, 8.9865e+01, 3.0437e+02, 9.4407e+01],\n",
            "        [1.6884e+02, 3.6117e+02, 1.7518e+02, 3.6674e+02],\n",
            "        [1.7252e+02, 1.3422e+02, 3.1090e+02, 3.7945e+02],\n",
            "        [2.7676e+02, 3.5799e+02, 2.8305e+02, 3.6152e+02],\n",
            "        [2.8934e+02, 8.9790e+01, 2.9420e+02, 9.4152e+01],\n",
            "        [1.4734e+02, 3.4272e+02, 1.5268e+02, 3.4825e+02],\n",
            "        [0.0000e+00, 2.2485e+02, 5.2629e+01, 4.8530e+02],\n",
            "        [1.7127e+01, 2.7622e+01, 1.4291e+02, 2.1716e+02],\n",
            "        [2.1193e+02, 1.0381e+02, 2.1727e+02, 1.0799e+02],\n",
            "        [2.7552e+02, 8.6974e+01, 2.8099e+02, 9.1380e+01],\n",
            "        [5.2845e+01, 2.4696e+02, 3.8185e+02, 3.6484e+02],\n",
            "        [2.9743e+02, 9.0089e+01, 3.0238e+02, 9.4267e+01],\n",
            "        [1.4920e+02, 3.4643e+02, 1.5454e+02, 3.5178e+02],\n",
            "        [2.1516e+02, 9.8632e+01, 2.2060e+02, 1.0272e+02],\n",
            "        [0.0000e+00, 1.4897e+02, 2.6447e+02, 2.7766e+02],\n",
            "        [1.5115e+02, 3.4806e+02, 1.5679e+02, 3.5331e+02],\n",
            "        [2.8738e+02, 8.8935e+01, 2.9232e+02, 9.3350e+01],\n",
            "        [1.9200e+02, 1.5392e+02, 1.9745e+02, 1.5850e+02],\n",
            "        [2.9958e+02, 8.7260e+01, 3.0455e+02, 9.1745e+01],\n",
            "        [1.9412e+02, 1.5432e+02, 2.0083e+02, 1.5889e+02],\n",
            "        [2.7809e+02, 3.5581e+02, 2.8415e+02, 3.5929e+02],\n",
            "        [1.7836e+02, 3.6011e+02, 1.8352e+02, 3.6564e+02],\n",
            "        [2.9610e+02, 9.9075e+01, 4.0000e+02, 3.3005e+02],\n",
            "        [1.8761e+02, 1.4993e+02, 1.9386e+02, 1.5461e+02],\n",
            "        [2.1517e+02, 1.0119e+02, 2.2075e+02, 1.0541e+02],\n",
            "        [0.0000e+00, 4.8642e-01, 4.0000e+02, 1.4429e+02],\n",
            "        [1.8971e+02, 1.5059e+02, 1.9511e+02, 1.5518e+02],\n",
            "        [2.8538e+02, 8.8689e+01, 2.9032e+02, 9.3190e+01],\n",
            "        [7.0700e-01, 2.0116e+02, 2.8301e+02, 4.0403e+02],\n",
            "        [1.1548e+01, 2.2500e+02, 1.0278e+02, 4.7411e+02],\n",
            "        [0.0000e+00, 0.0000e+00, 4.0000e+02, 4.0378e+02],\n",
            "        [1.4876e+02, 3.4078e+02, 1.5397e+02, 3.4654e+02],\n",
            "        [9.1230e+01, 1.5722e+02, 3.7073e+02, 3.5372e+02],\n",
            "        [2.8974e+02, 8.7592e+01, 2.9473e+02, 9.2216e+01],\n",
            "        [3.6800e+02, 0.0000e+00, 4.0000e+02, 3.9413e+01],\n",
            "        [2.1679e+02, 1.0086e+02, 2.2217e+02, 1.0507e+02],\n",
            "        [1.4940e+02, 3.4846e+02, 1.5482e+02, 3.5366e+02],\n",
            "        [0.0000e+00, 9.4904e+01, 3.1661e+02, 2.0189e+02],\n",
            "        [8.9346e+01, 2.9411e+02, 3.9794e+02, 4.2262e+02],\n",
            "        [2.7743e+02, 8.7868e+01, 2.8239e+02, 9.2532e+01],\n",
            "        [0.0000e+00, 3.0347e+02, 2.9838e+02, 4.0901e+02],\n",
            "        [2.1681e+02, 9.8089e+01, 2.2203e+02, 1.0220e+02],\n",
            "        [2.1395e+02, 9.9265e+01, 2.1912e+02, 1.0346e+02],\n",
            "        [2.7318e+02, 8.8709e+01, 2.7849e+02, 9.3335e+01],\n",
            "        [1.9402e+02, 1.4655e+02, 1.9821e+02, 1.5137e+02],\n",
            "        [1.7086e+02, 3.6165e+02, 1.7714e+02, 3.6755e+02],\n",
            "        [8.3569e+01, 2.0920e+02, 2.0928e+02, 4.0794e+02],\n",
            "        [2.9131e+02, 7.4718e-01, 4.0000e+02, 2.0719e+02],\n",
            "        [1.5798e+02, 1.0288e+02, 1.6290e+02, 1.0895e+02],\n",
            "        [2.8725e+02, 8.7076e+01, 2.9225e+02, 9.1552e+01],\n",
            "        [1.1248e+02, 1.7117e+02, 4.0000e+02, 2.7720e+02],\n",
            "        [2.7936e+02, 8.7493e+01, 2.8374e+02, 9.2358e+01],\n",
            "        [1.4727e+02, 3.4834e+02, 1.5258e+02, 3.5353e+02],\n",
            "        [2.8530e+02, 8.6882e+01, 2.9022e+02, 9.1349e+01],\n",
            "        [2.0619e+02, 9.7810e+01, 2.1150e+02, 1.0236e+02],\n",
            "        [2.7571e+02, 3.6131e+02, 2.8107e+02, 3.6556e+02],\n",
            "        [1.4721e+02, 3.4580e+02, 1.5248e+02, 3.5107e+02],\n",
            "        [1.3710e+02, 3.9552e+02, 4.0000e+02, 4.6086e+02],\n",
            "        [3.6009e+01, 2.6978e+02, 1.7454e+02, 4.6589e+02],\n",
            "        [1.9343e+02, 1.5263e+02, 1.9902e+02, 1.5707e+02],\n",
            "        [1.5118e+02, 3.4432e+02, 1.5655e+02, 3.5007e+02],\n",
            "        [1.5326e+02, 9.9640e+01, 1.5917e+02, 1.0481e+02],\n",
            "        [2.7685e+02, 3.5654e+02, 2.8309e+02, 3.5998e+02],\n",
            "        [2.1014e+02, 1.0353e+02, 2.1544e+02, 1.0781e+02],\n",
            "        [2.7830e+02, 9.0001e+01, 2.8360e+02, 9.4953e+01],\n",
            "        [2.9937e+02, 8.2718e+01, 3.0412e+02, 8.7227e+01],\n",
            "        [2.2818e+02, 1.4137e+02, 3.5533e+02, 3.6051e+02],\n",
            "        [2.1196e+02, 1.0246e+02, 2.1608e+02, 1.0716e+02]],\n",
            "       grad_fn=<StackBackward0>), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1,\n",
            "        2, 1, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
            "        1, 1, 2, 2]), 'scores': tensor([0.4715, 0.4627, 0.4558, 0.4543, 0.4507, 0.4400, 0.4325, 0.4275, 0.4235,\n",
            "        0.4223, 0.4214, 0.4174, 0.4161, 0.4160, 0.4137, 0.4127, 0.4078, 0.4064,\n",
            "        0.4030, 0.4015, 0.4004, 0.4004, 0.4001, 0.3985, 0.3984, 0.3978, 0.3977,\n",
            "        0.3962, 0.3960, 0.3957, 0.3956, 0.3948, 0.3947, 0.3941, 0.3938, 0.3928,\n",
            "        0.3927, 0.3923, 0.3917, 0.3917, 0.3915, 0.3913, 0.3907, 0.3907, 0.3905,\n",
            "        0.3903, 0.3892, 0.3889, 0.3886, 0.3885, 0.3878, 0.3878, 0.3870, 0.3869,\n",
            "        0.3864, 0.3861, 0.3859, 0.3859, 0.3857, 0.3856, 0.3854, 0.3852, 0.3852,\n",
            "        0.3844, 0.3843, 0.3843, 0.3841, 0.3835, 0.3835, 0.3832, 0.3829, 0.3827,\n",
            "        0.3826, 0.3824, 0.3823, 0.3822, 0.3819, 0.3817, 0.3815, 0.3812, 0.3810,\n",
            "        0.3802, 0.3797, 0.3795, 0.3791, 0.3791, 0.3790, 0.3787, 0.3785, 0.3784,\n",
            "        0.3783, 0.3781, 0.3778, 0.3773, 0.3770, 0.3770, 0.3768, 0.3767, 0.3766,\n",
            "        0.3764], grad_fn=<IndexBackward0>)}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YARNAxzypiso",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29f4099e-07c6-402f-c1c2-e1b78decadba"
      },
      "source": [
        "# use our dataset and defined transformations\n",
        "dataset = CansDataset(root, train=True)\n",
        "dataset_test = CansDataset(root, train=False)\n",
        "print(\"Oriignal dataset lengths are:\")\n",
        "print(\"training:\",len(dataset))\n",
        "print(\"test:\",len(dataset_test))\n",
        "\n",
        "\n",
        "# split the dataset in train and test set\n",
        "torch.manual_seed(1)\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "indices_test = torch.randperm(len(dataset_test)).tolist()\n",
        "\n",
        "dataset = torch.utils.data.Subset(dataset, indices[:100])          #indices[:-973])\n",
        "dataset_test = torch.utils.data.Subset(dataset_test,indices_test[:10]) #indices[-200:])\n",
        "\n",
        "print(\"Augmented dataset lengths are:\")\n",
        "print(\"training:\",len(dataset))\n",
        "print(\"test:\",len(dataset_test))\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
        "    collate_fn=scripts_colab.utils.collate_fn)\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
        "    collate_fn=scripts_colab.utils.collate_fn)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oriignal dataset lengths are:\n",
            "training: 1273\n",
            "test: 319\n",
            "Augmented dataset lengths are:\n",
            "training: 100\n",
            "test: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ7si63rpiso",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9e3a9a6-1c37-44c3-a834-be6f5cfc0f62"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(\"device:\",device)\n",
        "\n",
        "# our dataset has three classes only - background, beer and coke\n",
        "num_classes = 3\n",
        "\n",
        "# get the model using our helper function\n",
        "#model = get_instance_segmentation_model(num_classes)\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# and a learning rate scheduler which decreases the learning rate by\n",
        "# 10x every 3 epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                               step_size=3,\n",
        "                                               gamma=0.1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SPmJwk6pisp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1d79c26-ebd3-49ee-9608-78eca56144fb"
      },
      "source": [
        "# let's train it for 10 epochs\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "num_epochs = 1\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    #evaluate(model, data_loader_test, device=device)  \n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0]  [ 0/50]  eta: 0:01:39  lr: 0.000107  loss: 1.4859 (1.4859)  loss_classifier: 1.1622 (1.1622)  loss_box_reg: 0.3203 (0.3203)  loss_objectness: 0.0020 (0.0020)  loss_rpn_box_reg: 0.0013 (0.0013)  time: 1.9829  data: 0.4008  max mem: 2115\n",
            "Epoch: [0]  [10/50]  eta: 0:00:54  lr: 0.001126  loss: 0.6184 (0.8710)  loss_classifier: 0.4242 (0.6000)  loss_box_reg: 0.2479 (0.2619)  loss_objectness: 0.0020 (0.0065)  loss_rpn_box_reg: 0.0017 (0.0027)  time: 1.3595  data: 0.0475  max mem: 2383\n",
            "Epoch: [0]  [20/50]  eta: 0:00:39  lr: 0.002146  loss: 0.4289 (0.6348)  loss_classifier: 0.2032 (0.3968)  loss_box_reg: 0.2109 (0.2314)  loss_objectness: 0.0005 (0.0045)  loss_rpn_box_reg: 0.0017 (0.0021)  time: 1.3005  data: 0.0115  max mem: 2383\n",
            "Epoch: [0]  [30/50]  eta: 0:00:26  lr: 0.003165  loss: 0.3613 (0.5214)  loss_classifier: 0.1114 (0.2958)  loss_box_reg: 0.2040 (0.2195)  loss_objectness: 0.0003 (0.0040)  loss_rpn_box_reg: 0.0016 (0.0021)  time: 1.3065  data: 0.0110  max mem: 2383\n",
            "Epoch: [0]  [40/50]  eta: 0:00:13  lr: 0.004184  loss: 0.1696 (0.4389)  loss_classifier: 0.0580 (0.2382)  loss_box_reg: 0.1219 (0.1943)  loss_objectness: 0.0009 (0.0044)  loss_rpn_box_reg: 0.0015 (0.0020)  time: 1.3099  data: 0.0115  max mem: 2383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVqjXkRPpisq"
      },
      "source": [
        "evaluate(model, data_loader_test, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KkPWoCdpisq"
      },
      "source": [
        "# pick one image from the test set\n",
        "img, _ = dataset_test[0]\n",
        "# put the model in evaluation mode\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    prediction = model([img.to(device)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N41bD3cjpisr"
      },
      "source": [
        "prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-OBl78Upisr"
      },
      "source": [
        "# Plotting code\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "\n",
        "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
        "\n",
        "\n",
        "def show(imgs):\n",
        "    if not isinstance(imgs, list):\n",
        "        imgs = [imgs]\n",
        "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
        "    for i, img in enumerate(imgs):\n",
        "        img = img.detach()\n",
        "        img = F.to_pil_image(img)\n",
        "        axs[0, i].imshow(np.asarray(img))\n",
        "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1BnpfvPpisr"
      },
      "source": [
        "\n",
        "\n",
        "score_threshold = .8\n",
        "cans_with_boxes = []\n",
        "model.eval()\n",
        "ALL_TEST_IMAGES = len(dataset_test)\n",
        "HALF_TEST_IMAGES = ALL_TEST_IMAGES/2\n",
        "THREE_IMAGES = 3\n",
        "for i in range(ALL_TEST_IMAGES):\n",
        "    img, _ = dataset_test[i]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model([img.to(device)])[0] # prediction\n",
        "        img = img.mul(255).type(torch.uint8) # Bring to 0-255 from 0-1 and convert to usable type\n",
        "\n",
        "        # extract boxes and scores for each color box\n",
        "        green_boxes = {'boxes':output['boxes'][output['labels']==1],  'scores': output['scores'][output['labels']==1]}\n",
        "        red_boxes =   {'boxes':output['boxes'][output['labels']==2],  'scores': output['scores'][output['labels']==2]}\n",
        "        \n",
        "\n",
        "        # only extract valid boxes\n",
        "        valid_green_boxes = green_boxes['boxes'][green_boxes['scores'] > score_threshold]\n",
        "        valid_red_boxes = red_boxes['boxes'][red_boxes['scores'] > score_threshold]\n",
        "\n",
        "        # concat to one torch array\n",
        "        boxes = torch.cat((valid_green_boxes, valid_red_boxes),0)\n",
        "\n",
        "        # Add correct number of green and red boxes\n",
        "        colors = [len(valid_green_boxes)*['green'], len(valid_red_boxes)*['red']]\n",
        "        colors = [item for sublist in colors for item in sublist] # flatten list\n",
        "\n",
        "\n",
        "        # Prepare for plotting\n",
        "        draw = torchvision.utils.draw_bounding_boxes(img, boxes,colors=colors, width=4)\n",
        "        show(draw)\n",
        "        cans_with_boxes.append(draw)\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "# all in one plot\n",
        "show(cans_with_boxes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "989SdMb9piss"
      },
      "source": [
        "# If error with torch not releasing memory, delete model and empty cache. Start over\n",
        "#del model\n",
        "#torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}