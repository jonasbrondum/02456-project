{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing of objection detection network\n",
    "Based on Faster-RCNN and this guide: https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%shell\n",
    "\n",
    "# pip install cython\n",
    "# # Install pycocotools, the version by default in Colab\n",
    "# # has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354\n",
    "# pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and custom scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "sys.path.append(os.getcwd() + \"/..\" + \"/scripts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class with new __getitem__ function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "import transforms as T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CansDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, train=True):\n",
    "        if train is True:\n",
    "            self.root = root + \"/video1/train/\"\n",
    "        else:\n",
    "            self.root = root +  \"/video1/test/\"\n",
    "\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(self.root,\"frames\"))))\n",
    "        self.bbox = list(sorted(os.listdir(os.path.join(self.root,\"boundingboxes\"))))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # load images and bboxes\n",
    "        img_path = os.path.join(self.root, \"frames\", self.imgs[idx])\n",
    "        bbox_path = os.path.join(self.root, \"boundingboxes\", self.bbox[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img=np.array(img)\n",
    "        img=torch.tensor(img)/255\n",
    "        img=img.permute(2,0,1)\n",
    "\n",
    "        bbox = []\n",
    "        label = []\n",
    "        with open(bbox_path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.split(\" \")\n",
    "                id = line[0] # class label, 1=beer, 2=cola, 0=background\n",
    "                id = 1 if id == 'beer' else 2\n",
    "                xmin = float(line[1])\n",
    "                ymin = float(line[2])\n",
    "                xmax = float(line[3])\n",
    "                ymax = float(line[4])\n",
    "                bbox.append([xmin, ymin, xmax, ymax])\n",
    "                label.append(id)\n",
    "        bbox = torch.as_tensor(bbox, dtype=torch.int64)\n",
    "        labels = torch.as_tensor(label, dtype=torch.int64) #torch.ones((num_objs, ), dtype=torch.float32)\n",
    "        image_id = torch.tensor([idx],dtype=torch.int64)\n",
    "        area = (bbox[:, 3] - bbox[:, 1]) * (bbox[:, 2] - bbox[:, 0])\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = bbox\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/andreasgp/MEGAsync/DTU/9. Semester/Deep Learning/object-tracking-project/02456-project/notebooks\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#root = os.getcwd()+'data/'#os.getcwd() + '/..' + '/data/'\n",
    "sys.path.append(os.getcwd() + \"/..\" + \"/scripts\")\n",
    "print(os.getcwd())\n",
    "os.chdir(os.getcwd()+\"/..\")\n",
    "\n",
    "#print(root)\n",
    "root = os.getcwd() + \"/data\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding pretrained model and modify numbers of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# load a model pre-trained pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 3  # 1 class (person) + background\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instance_segmentation_model(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64387/1320764867.py:45: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  bbox = torch.as_tensor(bbox, dtype=torch.int64)\n",
      "/tmp/ipykernel_64387/1320764867.py:45: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  bbox = torch.as_tensor(bbox, dtype=torch.int64)\n",
      "/tmp/ipykernel_64387/1320764867.py:45: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  bbox = torch.as_tensor(bbox, dtype=torch.int64)\n",
      "/tmp/ipykernel_64387/1320764867.py:45: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  bbox = torch.as_tensor(bbox, dtype=torch.int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3804, 0.3882, 0.3647,  ..., 0.3647, 0.3843, 0.3922],\n",
      "         [0.3843, 0.3490, 0.3294,  ..., 0.3725, 0.3922, 0.3961],\n",
      "         [0.3765, 0.3373, 0.3529,  ..., 0.3725, 0.3922, 0.3961],\n",
      "         ...,\n",
      "         [0.1059, 0.1059, 0.1020,  ..., 0.0863, 0.0863, 0.0902],\n",
      "         [0.1059, 0.1059, 0.1020,  ..., 0.0863, 0.0863, 0.0902],\n",
      "         [0.1059, 0.1059, 0.1020,  ..., 0.0863, 0.0863, 0.0902]],\n",
      "\n",
      "        [[0.3294, 0.3412, 0.3098,  ..., 0.3059, 0.3216, 0.3216],\n",
      "         [0.3373, 0.3020, 0.2745,  ..., 0.3137, 0.3294, 0.3255],\n",
      "         [0.3176, 0.2824, 0.2902,  ..., 0.3137, 0.3294, 0.3255],\n",
      "         ...,\n",
      "         [0.1098, 0.1098, 0.1059,  ..., 0.0902, 0.0863, 0.0902],\n",
      "         [0.1098, 0.1098, 0.1059,  ..., 0.0902, 0.0863, 0.0902],\n",
      "         [0.1098, 0.1098, 0.1059,  ..., 0.0902, 0.0863, 0.0902]],\n",
      "\n",
      "        [[0.2549, 0.2549, 0.2078,  ..., 0.1843, 0.1882, 0.1804],\n",
      "         [0.2510, 0.2078, 0.1725,  ..., 0.1922, 0.1961, 0.1843],\n",
      "         [0.2275, 0.1804, 0.1882,  ..., 0.1922, 0.1961, 0.1843],\n",
      "         ...,\n",
      "         [0.0863, 0.0863, 0.0824,  ..., 0.0706, 0.0784, 0.0824],\n",
      "         [0.0863, 0.0863, 0.0824,  ..., 0.0706, 0.0784, 0.0824],\n",
      "         [0.0863, 0.0863, 0.0824,  ..., 0.0706, 0.0784, 0.0824]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasgp/.local/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[ 48.9748,  76.4988,  52.4558,  80.8412],\n",
      "        [216.0179, 169.6655, 219.3163, 173.5281],\n",
      "        [163.2359, 202.8737, 166.5454, 206.4323],\n",
      "        [ 48.9596,  74.9428,  52.4041,  79.3189],\n",
      "        [365.5294, 160.9054, 369.8692, 165.2826],\n",
      "        [ 47.5540,  76.4168,  51.4282,  80.9287],\n",
      "        [ 46.3876,  72.6617,  50.0949,  77.8197],\n",
      "        [ 47.4495,  74.5400,  51.2161,  79.0829],\n",
      "        [217.4637, 170.2014, 220.6037, 173.8633],\n",
      "        [ 52.1910,  75.3912,  55.7675,  79.7023],\n",
      "        [323.9361, 103.7627, 327.5754, 107.9266],\n",
      "        [ 52.1118,  73.7258,  55.7016,  77.9808],\n",
      "        [ 50.9825,  73.5484,  54.4292,  77.9100],\n",
      "        [ 50.6271,  76.5666,  54.1327,  80.9554],\n",
      "        [329.8467,  96.2168, 333.7085, 100.7062],\n",
      "        [216.0104, 171.4740, 219.1762, 175.1339],\n",
      "        [ 50.8987,  71.7445,  54.3731,  76.2274],\n",
      "        [242.0988, 103.8065, 245.1979, 108.3394],\n",
      "        [234.4603, 170.0946, 237.4371, 173.6755],\n",
      "        [164.8206, 201.2403, 168.2403, 205.6383],\n",
      "        [164.6417, 203.1063, 167.9913, 206.9838],\n",
      "        [328.4774, 110.1174, 331.6632, 114.0770],\n",
      "        [ 51.9875,  71.9079,  55.6112,  76.3542],\n",
      "        [167.5878, 201.9372, 171.0737, 206.3705],\n",
      "        [166.0588, 202.0311, 169.5997, 206.5850],\n",
      "        [161.8130, 204.1043, 165.5106, 208.1583],\n",
      "        [319.2482,  72.1953, 323.1187,  77.5621],\n",
      "        [330.1554, 110.5538, 333.3240, 114.3667],\n",
      "        [235.6985, 168.5071, 238.6887, 172.2802],\n",
      "        [364.3838, 159.8190, 368.3712, 164.1570],\n",
      "        [131.7910,  88.3236, 135.5837,  92.7012],\n",
      "        [175.2798, 203.7210, 178.3528, 208.5615],\n",
      "        [215.9059, 172.7631, 219.0844, 176.3569],\n",
      "        [246.0125, 105.3255, 249.3000, 110.3413],\n",
      "        [241.3634, 173.9677, 245.8246, 177.1427],\n",
      "        [  0.0000, 183.5066, 216.9835, 265.6828],\n",
      "        [217.4466, 171.5731, 220.4550, 175.2460],\n",
      "        [ 96.8834,  68.3202, 100.6093,  73.1388],\n",
      "        [120.9906, 154.8935, 124.2284, 160.0904],\n",
      "        [ 86.3695, 195.6472,  89.5271, 199.7589],\n",
      "        [217.5424, 172.8593, 220.4238, 176.4156],\n",
      "        [234.0726, 195.4648, 237.8307, 200.1486],\n",
      "        [235.7372, 170.6588, 238.6903, 174.4038],\n",
      "        [251.3081, 172.6040, 254.8102, 177.1671],\n",
      "        [242.9559,  99.2721, 246.5065, 104.3289],\n",
      "        [225.4319, 169.2818, 228.2968, 173.3417],\n",
      "        [163.2996, 200.2784, 166.8694, 204.5360],\n",
      "        [362.7371, 149.3227, 366.8642, 154.4917],\n",
      "        [234.2870, 168.1434, 237.3170, 171.7500],\n",
      "        [278.6335, 172.3416, 282.4261, 177.3700],\n",
      "        [323.4946,  71.7570, 327.1041,  76.9281],\n",
      "        [314.5511, 216.6099, 319.2674, 220.8288],\n",
      "        [328.3270, 108.3953, 331.5909, 112.2685],\n",
      "        [269.9700,  87.3087, 273.3032,  92.3079],\n",
      "        [163.5275, 204.7380, 166.5370, 208.3670],\n",
      "        [161.7300, 202.5108, 165.5064, 206.4283],\n",
      "        [324.0016, 100.9362, 328.6464, 104.7768],\n",
      "        [221.6976, 169.9705, 224.8292, 173.8436],\n",
      "        [276.9015, 155.5570, 281.3753, 160.5461],\n",
      "        [331.4228,  97.9312, 334.9568, 102.0818],\n",
      "        [258.9720, 161.2977, 262.6507, 166.2697],\n",
      "        [174.1169, 206.1165, 177.4171, 209.9201],\n",
      "        [242.0980, 101.7896, 245.2821, 106.2442],\n",
      "        [ 85.0698, 195.5526,  88.3164, 199.8389],\n",
      "        [232.7645, 169.9675, 235.8829, 173.5747],\n",
      "        [231.2341, 167.1833, 234.3454, 171.3677],\n",
      "        [325.6618,  72.1877, 329.1933,  77.0458],\n",
      "        [326.8298, 110.0814, 330.0853, 114.3357],\n",
      "        [315.5654,  71.1309, 320.2548,  75.7387],\n",
      "        [ 82.5448, 198.3846,  86.5332, 203.2492],\n",
      "        [240.7103, 174.0314, 244.6139, 178.2565],\n",
      "        [161.7441, 197.8782, 165.6991, 202.5645],\n",
      "        [225.4495, 171.0133, 228.3882, 174.9520],\n",
      "        [277.5354, 171.1464, 281.4341, 176.3172],\n",
      "        [244.2761, 102.2185, 247.4280, 107.2205],\n",
      "        [245.8391, 103.8160, 249.0317, 108.9039],\n",
      "        [157.6341, 209.5819, 160.7762, 212.8947],\n",
      "        [243.9597,  85.9902, 248.0423,  90.5526],\n",
      "        [321.0935,  71.7184, 324.6403,  76.8769],\n",
      "        [168.3281, 203.4195, 171.8582, 207.3087],\n",
      "        [217.5370, 174.3083, 220.4190, 177.8624],\n",
      "        [137.9654,  91.1808, 141.7437,  95.7337],\n",
      "        [244.2805, 104.0932, 247.5265, 109.0573],\n",
      "        [318.2403,  70.9426, 322.0400,  76.4408],\n",
      "        [174.3737, 207.5546, 177.5633, 211.2063],\n",
      "        [201.5525,  99.8639, 400.0000, 188.4556],\n",
      "        [233.9805, 165.6474, 237.1409, 169.4275],\n",
      "        [329.3768,  69.6644, 333.3183,  75.2966],\n",
      "        [323.9867,  73.1612, 327.6681,  78.1569],\n",
      "        [  0.0000, 172.2852, 293.3386, 300.0000],\n",
      "        [250.4015, 155.2624, 254.6218, 160.6861],\n",
      "        [244.4183, 105.6789, 247.7957, 110.5214],\n",
      "        [230.6864, 171.5717, 233.8523, 175.2615],\n",
      "        [136.2430,  89.3346, 139.9672,  93.6910],\n",
      "        [237.1598, 169.0383, 240.1052, 172.8966],\n",
      "        [235.3861, 165.7576, 238.6683, 169.7350],\n",
      "        [316.7827,  71.6415, 321.0255,  76.9627],\n",
      "        [175.2473, 206.1949, 178.3876, 210.8145],\n",
      "        [128.8150,  76.6456, 132.8490,  81.0114],\n",
      "        [240.0382,  87.1353, 243.6754,  91.5631]], grad_fn=<StackBackward0>), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1]), 'scores': tensor([0.6399, 0.6367, 0.6336, 0.6313, 0.6253, 0.6227, 0.6142, 0.6115, 0.6095,\n",
      "        0.6088, 0.6083, 0.6077, 0.6057, 0.6054, 0.6036, 0.6030, 0.6016, 0.6014,\n",
      "        0.6012, 0.6009, 0.6002, 0.6001, 0.5979, 0.5963, 0.5953, 0.5945, 0.5942,\n",
      "        0.5933, 0.5933, 0.5932, 0.5930, 0.5930, 0.5907, 0.5901, 0.5899, 0.5895,\n",
      "        0.5887, 0.5883, 0.5880, 0.5878, 0.5866, 0.5861, 0.5858, 0.5856, 0.5854,\n",
      "        0.5848, 0.5845, 0.5843, 0.5838, 0.5833, 0.5833, 0.5831, 0.5823, 0.5822,\n",
      "        0.5820, 0.5818, 0.5815, 0.5810, 0.5809, 0.5806, 0.5806, 0.5801, 0.5800,\n",
      "        0.5800, 0.5796, 0.5795, 0.5789, 0.5788, 0.5782, 0.5781, 0.5779, 0.5772,\n",
      "        0.5770, 0.5769, 0.5765, 0.5764, 0.5764, 0.5764, 0.5760, 0.5759, 0.5753,\n",
      "        0.5750, 0.5749, 0.5748, 0.5745, 0.5744, 0.5741, 0.5738, 0.5736, 0.5734,\n",
      "        0.5731, 0.5731, 0.5722, 0.5719, 0.5718, 0.5716, 0.5714, 0.5713, 0.5713,\n",
      "        0.5710], grad_fn=<IndexBackward0>)}, {'boxes': tensor([[263.6148, 318.6812, 268.1297, 324.5348],\n",
      "        [110.0130, 127.2880, 114.3443, 133.4511],\n",
      "        [108.0238, 143.7222, 112.5522, 148.5651],\n",
      "        [107.9285, 127.1934, 112.3563, 133.2845],\n",
      "        [298.5438, 198.8084, 303.4187, 204.1969],\n",
      "        [131.9245, 218.6357, 136.3584, 223.6322],\n",
      "        [130.2239, 221.1054, 134.7639, 225.9377],\n",
      "        [133.5362, 219.2921, 138.2584, 224.1442],\n",
      "        [175.9196, 318.3496, 180.4353, 325.2731],\n",
      "        [261.9664, 317.8388, 266.7167, 324.2243],\n",
      "        [177.5074, 317.7810, 182.0449, 324.4499],\n",
      "        [198.0561, 322.8427, 202.1238, 327.9337],\n",
      "        [133.5374, 217.5729, 138.6212, 222.7513],\n",
      "        [135.1412, 218.4212, 140.1598, 223.2380],\n",
      "        [131.9875, 221.1926, 136.4490, 225.9692],\n",
      "        [298.8346, 196.5062, 303.6117, 202.3077],\n",
      "        [105.4747, 127.0516, 110.0394, 132.9318],\n",
      "        [105.5108, 129.3977, 110.0051, 135.4015],\n",
      "        [131.6063, 213.9665, 136.3247, 220.3067],\n",
      "        [294.2588, 186.4322, 298.7386, 191.3736],\n",
      "        [264.9143, 306.4421, 269.2937, 311.4736],\n",
      "        [199.6975, 324.8927, 204.4470, 330.1826],\n",
      "        [110.0566, 143.6942, 114.5543, 148.3618],\n",
      "        [353.8074, 158.5844, 400.0000, 413.3677],\n",
      "        [293.1249, 199.1949, 297.5740, 204.8257],\n",
      "        [134.2684, 214.8935, 139.8656, 219.7494],\n",
      "        [191.2749, 323.2524, 196.6475, 328.4983],\n",
      "        [102.1778, 153.3851, 106.5853, 157.6345],\n",
      "        [113.0389, 143.8257, 117.5281, 148.0943],\n",
      "        [291.6985, 186.0992, 296.3797, 190.8833],\n",
      "        [214.4421, 176.4930, 218.8702, 181.2332],\n",
      "        [199.5268, 259.3919, 204.1648, 264.7527],\n",
      "        [191.1181, 320.6400, 196.3799, 326.0609],\n",
      "        [275.7311, 275.2117, 280.1398, 280.3351],\n",
      "        [139.2168, 131.9463, 143.6110, 136.9453],\n",
      "        [114.9131, 153.4703, 118.9094, 157.4485],\n",
      "        [222.1439, 352.6377, 226.7072, 357.4953],\n",
      "        [179.6983, 318.3217, 184.2012, 324.7157],\n",
      "        [263.7740, 320.5612, 268.1980, 326.5825],\n",
      "        [108.0617, 147.6734, 112.3576, 152.1286],\n",
      "        [259.6364, 319.8478, 264.3301, 326.6759],\n",
      "        [296.7410, 199.1731, 302.5274, 203.3958],\n",
      "        [139.2712, 129.9958, 143.6843, 134.9939],\n",
      "        [111.6951, 144.0011, 116.0635, 148.6062],\n",
      "        [141.1577, 138.5460, 145.5614, 143.9192],\n",
      "        [273.2658, 275.4814, 277.5425, 280.9206],\n",
      "        [117.5085, 133.4996, 122.3699, 139.4364],\n",
      "        [296.1532, 186.5371, 300.5138, 191.7506],\n",
      "        [177.6902, 322.2911, 182.2656, 328.5837],\n",
      "        [111.9317, 127.2693, 116.6140, 133.5123],\n",
      "        [278.1473, 275.0560, 282.6687, 279.9995],\n",
      "        [275.6191, 277.1363, 279.9871, 282.3622],\n",
      "        [131.9783, 213.3148, 138.1264, 218.2483],\n",
      "        [276.1081, 273.1300, 280.4817, 278.3241],\n",
      "        [189.5802, 323.1459, 194.7429, 328.3602],\n",
      "        [129.7922, 210.9359, 134.3969, 217.3044],\n",
      "        [201.6465, 258.6049, 206.3643, 263.9797],\n",
      "        [273.0304, 277.7260, 277.5092, 283.2277],\n",
      "        [273.4619, 273.2823, 277.5763, 278.8703],\n",
      "        [133.7136, 137.4600, 137.9035, 142.0673],\n",
      "        [292.4513, 190.0607, 297.1721, 195.4204],\n",
      "        [109.2103, 153.5135, 114.0853, 157.6482],\n",
      "        [108.0029, 129.5122, 112.4087, 135.7594],\n",
      "        [105.6791, 142.4773, 110.2201, 147.8924],\n",
      "        [297.4994, 186.5069, 301.9154, 192.0897],\n",
      "        [124.1112, 134.1741, 128.4018, 139.9933],\n",
      "        [135.0209, 212.7651, 140.0391, 218.3353],\n",
      "        [288.2973, 187.6690, 292.8976, 192.9140],\n",
      "        [133.6233, 221.5278, 138.2201, 226.2861],\n",
      "        [125.1893, 135.1652, 130.1360, 139.4751],\n",
      "        [112.6113, 142.3875, 117.2734, 146.6166],\n",
      "        [289.8848, 185.6757, 294.5331, 190.4670],\n",
      "        [297.5776, 183.6010, 301.9173, 189.3178],\n",
      "        [234.0164, 359.3035, 239.0839, 364.5504],\n",
      "        [281.0741, 278.1077, 285.5114, 282.9766],\n",
      "        [126.2778, 246.1317, 133.8935, 252.2364],\n",
      "        [137.2349, 132.0846, 141.6200, 136.9219],\n",
      "        [141.1611, 136.4569, 145.6758, 141.7085],\n",
      "        [105.7914, 131.5488, 110.2798, 137.5994],\n",
      "        [265.4076, 319.5678, 270.2135, 325.2658],\n",
      "        [280.8901, 271.9586, 285.6132, 277.0617],\n",
      "        [129.9815, 124.4075, 134.7448, 130.3995],\n",
      "        [107.8512, 140.2976, 112.5051, 145.8436],\n",
      "        [292.3053, 188.1104, 297.0633, 193.3202],\n",
      "        [288.3961, 185.2044, 293.0305, 190.0517],\n",
      "        [259.5287, 305.0009, 264.1088, 309.4429],\n",
      "        [279.6504, 278.0919, 284.0367, 282.9008],\n",
      "        [278.1839, 272.4677, 282.8409, 277.5551],\n",
      "        [113.7166, 145.6257, 117.8513, 149.9323],\n",
      "        [280.5381, 269.7394, 285.5439, 275.2364],\n",
      "        [288.4949, 189.9802, 292.8184, 195.2971],\n",
      "        [271.5139, 273.3302, 275.7277, 279.3080],\n",
      "        [281.0450, 274.7214, 285.5930, 279.5669],\n",
      "        [141.1217, 133.9842, 145.7956, 139.0612],\n",
      "        [118.0640, 125.8530, 122.9253, 131.6187],\n",
      "        [250.2333, 326.9604, 255.6879, 332.0548],\n",
      "        [223.6215, 353.0985, 228.3243, 357.9758],\n",
      "        [257.6545, 305.3234, 262.0424, 309.7914],\n",
      "        [138.1690, 129.1337, 143.5200, 133.5267],\n",
      "        [119.8774, 125.7936, 124.5217, 132.0123]], grad_fn=<StackBackward0>), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1]), 'scores': tensor([0.6317, 0.6268, 0.6267, 0.6236, 0.6209, 0.6201, 0.6177, 0.6174, 0.6174,\n",
      "        0.6148, 0.6136, 0.6130, 0.6120, 0.6115, 0.6114, 0.6113, 0.6113, 0.6109,\n",
      "        0.6106, 0.6081, 0.6076, 0.6072, 0.6064, 0.6061, 0.6055, 0.6047, 0.6046,\n",
      "        0.6045, 0.6041, 0.6036, 0.6033, 0.6029, 0.6028, 0.6026, 0.6024, 0.6024,\n",
      "        0.6020, 0.6019, 0.6007, 0.6005, 0.6002, 0.6000, 0.5995, 0.5995, 0.5987,\n",
      "        0.5984, 0.5981, 0.5971, 0.5971, 0.5970, 0.5965, 0.5963, 0.5962, 0.5958,\n",
      "        0.5957, 0.5956, 0.5954, 0.5951, 0.5949, 0.5949, 0.5948, 0.5946, 0.5945,\n",
      "        0.5944, 0.5942, 0.5940, 0.5939, 0.5935, 0.5927, 0.5923, 0.5921, 0.5920,\n",
      "        0.5920, 0.5918, 0.5917, 0.5916, 0.5913, 0.5913, 0.5912, 0.5907, 0.5907,\n",
      "        0.5906, 0.5905, 0.5905, 0.5905, 0.5904, 0.5903, 0.5903, 0.5902, 0.5902,\n",
      "        0.5902, 0.5901, 0.5900, 0.5900, 0.5898, 0.5897, 0.5897, 0.5896, 0.5896,\n",
      "        0.5896], grad_fn=<IndexBackward0>)}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "import utils\n",
    "import transforms as T\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = CansDataset(root, train=True)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "# For Training\n",
    "images,targets = next(iter(data_loader))\n",
    "images = list(image for image in images)\n",
    "\n",
    "\n",
    "print(images[0])\n",
    "\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "\n",
    "output = model(images,targets)   # Returns losses and detections\n",
    "# For inference\n",
    "model.eval()\n",
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "predictions = model(x)           # Returns predictions\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Do\n",
    "Fix get item så den kører med Jonas' data\\\n",
    "Få vores data til at kører med den pretrænede model\\\n",
    "Test forward pass\\\n",
    "Benyt Holgers split til at træne med og opnå fuld model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use our dataset and defined transformations\n",
    "dataset = CansDataset(root, train=True)\n",
    "dataset_test = CansDataset(root, train=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# split the dataset in train and test set\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has three classes only - background, beer and coke\n",
    "num_classes = 3\n",
    "\n",
    "# get the model using our helper function\n",
    "#model = get_instance_segmentation_model(num_classes)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64387/1320764867.py:45: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  bbox = torch.as_tensor(bbox, dtype=torch.int64)\n",
      "/tmp/ipykernel_64387/1320764867.py:45: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  bbox = torch.as_tensor(bbox, dtype=torch.int64)\n",
      "/tmp/ipykernel_64387/1320764867.py:45: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  bbox = torch.as_tensor(bbox, dtype=torch.int64)\n",
      "/tmp/ipykernel_64387/1320764867.py:45: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  bbox = torch.as_tensor(bbox, dtype=torch.int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/590]  eta: 2:12:41  lr: 0.000013  loss: 1.5829 (1.5829)  loss_classifier: 1.2614 (1.2614)  loss_box_reg: 0.3137 (0.3137)  loss_objectness: 0.0068 (0.0068)  loss_rpn_box_reg: 0.0010 (0.0010)  time: 13.4944  data: 0.2846\n"
     ]
    }
   ],
   "source": [
    "# let's train it for 10 epochs\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick one image from the test set\n",
    "img, _ = dataset_test[0]\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
