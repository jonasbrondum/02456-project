{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing of objection detection network\n",
    "Based on Faster-RCNN and this guide: https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%shell\n",
    "\n",
    "# pip install cython\n",
    "# # Install pycocotools, the version by default in Colab\n",
    "# # has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354\n",
    "# pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and custom scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "sys.path.append(os.getcwd() + \"/..\" + \"/scripts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class with new __getitem__ function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "import transforms as T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CansDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root,\"video1/train/frames\"))))\n",
    "        self.bbox = list(sorted(os.listdir(os.path.join(root,\"video1/train/boundingboxes\"))))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # load images and bboxes\n",
    "        img_path = os.path.join(self.root, \"video1/train/frames\", self.imgs[idx])\n",
    "        bbox_path = os.path.join(self.root, \"video1/train/boundingboxes\", self.bbox[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img=np.array(img)\n",
    "        img=torch.tensor(img)/255\n",
    "        img=img.permute(2,0,1)\n",
    "\n",
    "        bbox = []\n",
    "        label = []\n",
    "        with open(bbox_path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.split(\" \")\n",
    "                id = line[0] # class label, 1=beer, 2=cola, 0=background\n",
    "                id = 1 if id == 'beer' else 2\n",
    "                xmin = float(line[1])\n",
    "                ymin = float(line[2])\n",
    "                xmax = float(line[3])\n",
    "                ymax = float(line[4])\n",
    "                bbox.append([xmin, ymin, xmax, ymax])\n",
    "                label.append(id)\n",
    "        bbox = torch.as_tensor(bbox, dtype=torch.int64)\n",
    "        labels = torch.as_tensor(label, dtype=torch.int64) #torch.ones((num_objs, ), dtype=torch.float32)\n",
    "        image_id = torch.tensor([idx],dtype=torch.int64)\n",
    "        area = (bbox[:, 3] - bbox[:, 1]) * (bbox[:, 2] - bbox[:, 0])\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = bbox\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "\n",
    "\n",
    "        #img = pil2tensor(images[0])\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/andreasgp/MEGAsync/DTU/9. Semester/Deep Learning/object-tracking-project/02456-project/notebooks\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#root = os.getcwd()+'data/'#os.getcwd() + '/..' + '/data/'\n",
    "sys.path.append(os.getcwd() + \"/..\" + \"/scripts\")\n",
    "print(os.getcwd())\n",
    "os.chdir(os.getcwd()+\"/..\")\n",
    "\n",
    "#print(root)\n",
    "root = os.getcwd() + \"/data\"\n",
    "dataset = CansDataset(root)\n",
    "\n",
    "\n",
    "# img,target = dataset.__getitem__(907)\n",
    "# print(\"img\",img)\n",
    "# print(\"target\",target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding pretrained model and modify numbers of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# load a model pre-trained pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 3  # 1 class (person) + background\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63266/4023085708.py:41: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  bbox = torch.as_tensor(bbox, dtype=torch.int64)\n",
      "/tmp/ipykernel_63266/4023085708.py:41: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  bbox = torch.as_tensor(bbox, dtype=torch.int64)\n",
      "/tmp/ipykernel_63266/4023085708.py:41: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  bbox = torch.as_tensor(bbox, dtype=torch.int64)\n",
      "/tmp/ipykernel_63266/4023085708.py:41: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  bbox = torch.as_tensor(bbox, dtype=torch.int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3882, 0.3843, 0.3686,  ..., 0.2627, 0.2745, 0.2784],\n",
      "         [0.4000, 0.3333, 0.3059,  ..., 0.2627, 0.2706, 0.2745],\n",
      "         [0.3843, 0.3255, 0.3294,  ..., 0.2667, 0.2706, 0.2706],\n",
      "         ...,\n",
      "         [0.1059, 0.1059, 0.1059,  ..., 0.0745, 0.0784, 0.0824],\n",
      "         [0.1059, 0.1059, 0.1098,  ..., 0.0745, 0.0784, 0.0824],\n",
      "         [0.1059, 0.1098, 0.1098,  ..., 0.0745, 0.0784, 0.0824]],\n",
      "\n",
      "        [[0.3490, 0.3451, 0.3333,  ..., 0.2157, 0.2275, 0.2314],\n",
      "         [0.3608, 0.2941, 0.2627,  ..., 0.2157, 0.2235, 0.2275],\n",
      "         [0.3255, 0.2706, 0.2745,  ..., 0.2196, 0.2235, 0.2235],\n",
      "         ...,\n",
      "         [0.1098, 0.1098, 0.1098,  ..., 0.0784, 0.0824, 0.0863],\n",
      "         [0.1098, 0.1098, 0.1137,  ..., 0.0784, 0.0824, 0.0863],\n",
      "         [0.1098, 0.1137, 0.1137,  ..., 0.0784, 0.0824, 0.0863]],\n",
      "\n",
      "        [[0.2510, 0.2392, 0.2196,  ..., 0.1216, 0.1333, 0.1373],\n",
      "         [0.2627, 0.1882, 0.1529,  ..., 0.1216, 0.1294, 0.1333],\n",
      "         [0.2353, 0.1686, 0.1608,  ..., 0.1255, 0.1294, 0.1294],\n",
      "         ...,\n",
      "         [0.0863, 0.0863, 0.0863,  ..., 0.0588, 0.0627, 0.0667],\n",
      "         [0.0863, 0.0863, 0.0902,  ..., 0.0588, 0.0627, 0.0667],\n",
      "         [0.0863, 0.0902, 0.0902,  ..., 0.0588, 0.0627, 0.0667]]])\n",
      "typing.List[typing.Tuple[int, int]]\n",
      "tensor([[[0.3882, 0.3843, 0.3686,  ..., 0.2627, 0.2745, 0.2784],\n",
      "         [0.4000, 0.3333, 0.3059,  ..., 0.2627, 0.2706, 0.2745],\n",
      "         [0.3843, 0.3255, 0.3294,  ..., 0.2667, 0.2706, 0.2706],\n",
      "         ...,\n",
      "         [0.1059, 0.1059, 0.1059,  ..., 0.0745, 0.0784, 0.0824],\n",
      "         [0.1059, 0.1059, 0.1098,  ..., 0.0745, 0.0784, 0.0824],\n",
      "         [0.1059, 0.1098, 0.1098,  ..., 0.0745, 0.0784, 0.0824]],\n",
      "\n",
      "        [[0.3490, 0.3451, 0.3333,  ..., 0.2157, 0.2275, 0.2314],\n",
      "         [0.3608, 0.2941, 0.2627,  ..., 0.2157, 0.2235, 0.2275],\n",
      "         [0.3255, 0.2706, 0.2745,  ..., 0.2196, 0.2235, 0.2235],\n",
      "         ...,\n",
      "         [0.1098, 0.1098, 0.1098,  ..., 0.0784, 0.0824, 0.0863],\n",
      "         [0.1098, 0.1098, 0.1137,  ..., 0.0784, 0.0824, 0.0863],\n",
      "         [0.1098, 0.1137, 0.1137,  ..., 0.0784, 0.0824, 0.0863]],\n",
      "\n",
      "        [[0.2510, 0.2392, 0.2196,  ..., 0.1216, 0.1333, 0.1373],\n",
      "         [0.2627, 0.1882, 0.1529,  ..., 0.1216, 0.1294, 0.1333],\n",
      "         [0.2353, 0.1686, 0.1608,  ..., 0.1255, 0.1294, 0.1294],\n",
      "         ...,\n",
      "         [0.0863, 0.0863, 0.0863,  ..., 0.0588, 0.0627, 0.0667],\n",
      "         [0.0863, 0.0863, 0.0902,  ..., 0.0588, 0.0627, 0.0667],\n",
      "         [0.0863, 0.0902, 0.0902,  ..., 0.0588, 0.0627, 0.0667]]])\n",
      "tensor([[[0.3843, 0.3765, 0.3804,  ..., 0.2627, 0.2706, 0.2706],\n",
      "         [0.4039, 0.3255, 0.3098,  ..., 0.2706, 0.2784, 0.2784],\n",
      "         [0.3882, 0.3216, 0.3137,  ..., 0.2745, 0.2824, 0.2824],\n",
      "         ...,\n",
      "         [0.0980, 0.0980, 0.0980,  ..., 0.0627, 0.0627, 0.0627],\n",
      "         [0.0980, 0.0980, 0.0941,  ..., 0.0627, 0.0627, 0.0627],\n",
      "         [0.0980, 0.0980, 0.0941,  ..., 0.0627, 0.0627, 0.0627]],\n",
      "\n",
      "        [[0.3294, 0.3216, 0.3255,  ..., 0.2157, 0.2275, 0.2275],\n",
      "         [0.3490, 0.2706, 0.2549,  ..., 0.2235, 0.2353, 0.2353],\n",
      "         [0.3333, 0.2588, 0.2510,  ..., 0.2275, 0.2392, 0.2392],\n",
      "         ...,\n",
      "         [0.1020, 0.1020, 0.1020,  ..., 0.0667, 0.0667, 0.0667],\n",
      "         [0.1020, 0.1020, 0.0980,  ..., 0.0667, 0.0667, 0.0667],\n",
      "         [0.1020, 0.1020, 0.0980,  ..., 0.0667, 0.0667, 0.0667]],\n",
      "\n",
      "        [[0.2275, 0.2196, 0.2235,  ..., 0.1137, 0.1176, 0.1098],\n",
      "         [0.2471, 0.1686, 0.1490,  ..., 0.1216, 0.1255, 0.1176],\n",
      "         [0.2314, 0.1608, 0.1490,  ..., 0.1255, 0.1294, 0.1216],\n",
      "         ...,\n",
      "         [0.0784, 0.0784, 0.0784,  ..., 0.0471, 0.0471, 0.0471],\n",
      "         [0.0784, 0.0784, 0.0745,  ..., 0.0471, 0.0471, 0.0471],\n",
      "         [0.0784, 0.0784, 0.0745,  ..., 0.0471, 0.0471, 0.0471]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasgp/MEGAsync/DTU/9. Semester/Deep Learning/object-tracking-project/02456-project/env/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "typing.List[typing.Tuple[int, int]]\n",
      "tensor([[[0.1282, 0.5887, 0.2213,  ..., 0.3449, 0.0592, 0.3018],\n",
      "         [0.0652, 0.8235, 0.9055,  ..., 0.6729, 0.5918, 0.0409],\n",
      "         [0.0983, 0.7814, 0.8326,  ..., 0.8028, 0.3649, 0.2984],\n",
      "         ...,\n",
      "         [0.5545, 0.9740, 0.4885,  ..., 0.6818, 0.3860, 0.0414],\n",
      "         [0.9502, 0.3671, 0.5939,  ..., 0.1750, 0.5009, 0.0874],\n",
      "         [0.6049, 0.6741, 0.2731,  ..., 0.1026, 0.3659, 0.8614]],\n",
      "\n",
      "        [[0.5350, 0.7905, 0.2743,  ..., 0.1225, 0.4954, 0.6784],\n",
      "         [0.9291, 0.6579, 0.7402,  ..., 0.2481, 0.0947, 0.2566],\n",
      "         [0.0177, 0.5821, 0.0965,  ..., 0.8593, 0.9256, 0.1965],\n",
      "         ...,\n",
      "         [0.2383, 0.5356, 0.3392,  ..., 0.5606, 0.4862, 0.5269],\n",
      "         [0.8673, 0.6599, 0.7377,  ..., 0.3050, 0.7345, 0.9793],\n",
      "         [0.3035, 0.4061, 0.4279,  ..., 0.5140, 0.4490, 0.7564]],\n",
      "\n",
      "        [[0.2644, 0.2633, 0.9981,  ..., 0.5227, 0.5020, 0.2502],\n",
      "         [0.8142, 0.6570, 0.3960,  ..., 0.6870, 0.5365, 0.5096],\n",
      "         [0.2468, 0.9644, 0.0816,  ..., 0.9025, 0.2987, 0.9202],\n",
      "         ...,\n",
      "         [0.2863, 0.1204, 0.6259,  ..., 0.3944, 0.7739, 0.9578],\n",
      "         [0.3246, 0.9860, 0.0797,  ..., 0.3470, 0.6318, 0.1957],\n",
      "         [0.3831, 0.5469, 0.7054,  ..., 0.0914, 0.3367, 0.1022]]])\n",
      "tensor([[[6.9166e-01, 7.3887e-01, 6.0028e-01,  ..., 1.7711e-01,\n",
      "          7.9194e-01, 5.4032e-01],\n",
      "         [2.8356e-01, 9.4428e-02, 6.4963e-01,  ..., 2.5247e-01,\n",
      "          2.0222e-01, 5.0284e-01],\n",
      "         [5.9607e-01, 7.9150e-01, 9.2986e-01,  ..., 5.0030e-01,\n",
      "          1.6235e-01, 9.2178e-01],\n",
      "         ...,\n",
      "         [7.8948e-01, 8.3442e-01, 7.4241e-01,  ..., 2.7899e-01,\n",
      "          6.8754e-01, 2.4776e-01],\n",
      "         [7.3284e-01, 5.4536e-02, 7.9062e-01,  ..., 9.3516e-01,\n",
      "          3.3339e-01, 7.5522e-02],\n",
      "         [2.8468e-01, 5.6419e-01, 9.6079e-01,  ..., 7.2809e-01,\n",
      "          1.0034e-01, 1.7382e-01]],\n",
      "\n",
      "        [[1.9259e-01, 4.6504e-01, 6.4062e-01,  ..., 6.2942e-01,\n",
      "          9.3064e-01, 2.1783e-01],\n",
      "         [3.4368e-04, 5.0632e-01, 6.6722e-01,  ..., 3.4399e-01,\n",
      "          1.2475e-01, 9.7674e-01],\n",
      "         [3.3576e-01, 5.3333e-01, 1.6321e-01,  ..., 2.7700e-01,\n",
      "          8.6446e-01, 5.0882e-01],\n",
      "         ...,\n",
      "         [9.2743e-01, 6.0945e-01, 5.6448e-01,  ..., 4.0737e-01,\n",
      "          2.9518e-01, 4.6409e-01],\n",
      "         [8.7285e-01, 7.2160e-01, 4.8834e-01,  ..., 9.1743e-01,\n",
      "          8.1231e-01, 8.5837e-01],\n",
      "         [8.3099e-01, 5.4625e-02, 4.7984e-01,  ..., 2.1871e-01,\n",
      "          3.8288e-01, 8.5899e-01]],\n",
      "\n",
      "        [[4.8982e-01, 5.8801e-01, 1.4838e-01,  ..., 9.3490e-01,\n",
      "          6.1247e-01, 9.5694e-02],\n",
      "         [9.8751e-01, 8.6494e-01, 2.6443e-01,  ..., 2.7208e-01,\n",
      "          2.4578e-01, 2.7840e-01],\n",
      "         [6.2709e-01, 4.8893e-01, 2.3656e-01,  ..., 6.0076e-02,\n",
      "          9.7919e-01, 8.4279e-01],\n",
      "         ...,\n",
      "         [8.3399e-01, 3.4919e-01, 1.6639e-01,  ..., 5.9758e-01,\n",
      "          3.4677e-01, 5.5808e-01],\n",
      "         [1.5205e-01, 9.0750e-01, 8.4259e-01,  ..., 6.2008e-01,\n",
      "          2.4625e-01, 3.5112e-01],\n",
      "         [2.5831e-01, 7.7707e-01, 9.4361e-01,  ..., 7.8792e-01,\n",
      "          7.5009e-01, 6.4767e-01]]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#from engine import train_one_epoch, evaluate\n",
    "\n",
    "import utils\n",
    "import transforms as T\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = CansDataset(root)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "# For Training\n",
    "images,targets = next(iter(data_loader))\n",
    "images = list(image for image in images)\n",
    "\n",
    "\n",
    "print(images[0])\n",
    "\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "\n",
    "output = model(images,targets)   # Returns losses and detections\n",
    "# For inference\n",
    "model.eval()\n",
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "predictions = model(x)           # Returns predictions\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Do\n",
    "Fix get item så den kører med Jonas' data\\\n",
    "Få vores data til at kører med den pretrænede model\\\n",
    "Test forward pass\\\n",
    "Benyt Holgers split til at træne med og opnå fuld model\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
