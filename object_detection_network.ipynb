{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing of objection detection network\n",
    "Based on Faster-RCNN and this guide: https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%shell\n",
    "\n",
    "# pip install cython\n",
    "# # Install pycocotools, the version by default in Colab\n",
    "# # has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354\n",
    "# pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and custom scripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "sys.path.append(os.getcwd() + \"/..\" + \"/scripts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class with new __getitem__ function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "import transforms as T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CansDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, train=True):\n",
    "        if train is True:\n",
    "            self.root = root + \"/video1/train/\"\n",
    "        else:\n",
    "            self.root = root +  \"/video1/test/\"\n",
    "\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(self.root,\"frames\"))))\n",
    "        self.bbox = list(sorted(os.listdir(os.path.join(self.root,\"boundingboxes\"))))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # load images and bboxes\n",
    "        img_path = os.path.join(self.root, \"frames\", self.imgs[idx])\n",
    "        bbox_path = os.path.join(self.root, \"boundingboxes\", self.bbox[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img=np.array(img)\n",
    "        img=torch.tensor(img)/255\n",
    "        img=img.permute(2,0,1)\n",
    "\n",
    "        bbox = []\n",
    "        label = []\n",
    "        lines = 0\n",
    "        iscrowd = []\n",
    "        with open(bbox_path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.split(\" \")\n",
    "                id = line[0] # class label, 1=beer, 2=cola, 0=background\n",
    "                id = 1 if id == 'beer' else 2\n",
    "                xmin = float(line[1])\n",
    "                ymin = float(line[2])\n",
    "                xmax = float(line[3])\n",
    "                ymax = float(line[4])\n",
    "                bbox.append([xmin, ymin, xmax, ymax])\n",
    "                label.append(id)\n",
    "                lines += 1\n",
    "                iscrowd.append(False)\n",
    "\n",
    "        bbox = torch.as_tensor(bbox, dtype=torch.int64)\n",
    "        labels = torch.as_tensor(label, dtype=torch.int64) #torch.ones((num_objs, ), dtype=torch.float32)\n",
    "        image_id = torch.tensor([idx],dtype=torch.int64)\n",
    "        area = (bbox[:, 3] - bbox[:, 1]) * (bbox[:, 2] - bbox[:, 0])\n",
    "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "        #iscrowd = torch.zeros((lines,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = bbox\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        # if no boxes\n",
    "        # torch.zeros((0,4), dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/andreasgp/MEGAsync/DTU/9. Semester/Deep Learning/object-tracking-project/02456-project/notebooks\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#root = os.getcwd()+'data/'#os.getcwd() + '/..' + '/data/'\n",
    "sys.path.append(os.getcwd() + \"/..\" + \"/scripts\")\n",
    "print(os.getcwd())\n",
    "os.chdir(os.getcwd()+\"/..\")\n",
    "\n",
    "#print(root)\n",
    "root = os.getcwd() + \"/data\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding pretrained model and modify numbers of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# load a model pre-trained pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 3  # 1 class (person) + background\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instance_segmentation_model(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9449/3082487489.py:50: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  bbox = torch.as_tensor(bbox, dtype=torch.int64)\n",
      "/tmp/ipykernel_9449/3082487489.py:50: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  bbox = torch.as_tensor(bbox, dtype=torch.int64)\n",
      "/tmp/ipykernel_9449/3082487489.py:50: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  bbox = torch.as_tensor(bbox, dtype=torch.int64)\n",
      "/tmp/ipykernel_9449/3082487489.py:50: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  bbox = torch.as_tensor(bbox, dtype=torch.int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4039, 0.3804, 0.3922,  ..., 0.2706, 0.2706, 0.2667],\n",
      "         [0.4039, 0.3412, 0.3098,  ..., 0.2784, 0.2784, 0.2745],\n",
      "         [0.3922, 0.3294, 0.3176,  ..., 0.2824, 0.2824, 0.2784],\n",
      "         ...,\n",
      "         [0.1020, 0.1020, 0.1020,  ..., 0.0784, 0.0784, 0.0784],\n",
      "         [0.1020, 0.1020, 0.1020,  ..., 0.0784, 0.0784, 0.0784],\n",
      "         [0.1020, 0.1020, 0.1020,  ..., 0.0784, 0.0784, 0.0784]],\n",
      "\n",
      "        [[0.3294, 0.3098, 0.3216,  ..., 0.2275, 0.2275, 0.2235],\n",
      "         [0.3294, 0.2706, 0.2392,  ..., 0.2353, 0.2353, 0.2314],\n",
      "         [0.3216, 0.2588, 0.2392,  ..., 0.2392, 0.2392, 0.2353],\n",
      "         ...,\n",
      "         [0.1059, 0.1059, 0.1059,  ..., 0.0824, 0.0824, 0.0824],\n",
      "         [0.1059, 0.1059, 0.1059,  ..., 0.0824, 0.0824, 0.0824],\n",
      "         [0.1059, 0.1059, 0.1059,  ..., 0.0824, 0.0824, 0.0824]],\n",
      "\n",
      "        [[0.2627, 0.2314, 0.2275,  ..., 0.1176, 0.1176, 0.1059],\n",
      "         [0.2627, 0.1922, 0.1451,  ..., 0.1255, 0.1176, 0.1137],\n",
      "         [0.2431, 0.1725, 0.1412,  ..., 0.1294, 0.1216, 0.1098],\n",
      "         ...,\n",
      "         [0.0824, 0.0824, 0.0824,  ..., 0.0627, 0.0627, 0.0627],\n",
      "         [0.0824, 0.0824, 0.0824,  ..., 0.0627, 0.0627, 0.0627],\n",
      "         [0.0824, 0.0824, 0.0824,  ..., 0.0627, 0.0627, 0.0627]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreasgp/.local/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[ 61.2084, 146.3186, 323.1326, 290.9955],\n",
      "        [ 22.8236, 134.6014, 124.0131, 279.2125],\n",
      "        [  0.0000,  69.2209,  51.6782, 280.1263],\n",
      "        [ 77.9764,  65.7266,  81.8052,  70.2569],\n",
      "        [293.8410, 118.9181, 297.5344, 124.0548],\n",
      "        [  0.0000, 150.3896, 219.2576, 295.2558],\n",
      "        [ 69.1974,  99.2678, 278.2643, 232.8902],\n",
      "        [ 69.9732,  92.1933, 301.8970, 174.3298],\n",
      "        [ 97.1194, 132.8947, 354.7691, 209.5895],\n",
      "        [ 73.1813,  61.3903,  76.8940,  65.5782],\n",
      "        [  1.8352,  66.2828, 147.8917, 196.8068],\n",
      "        [ 67.7157,  72.5782, 322.2645, 146.5858],\n",
      "        [126.6452, 217.8990, 130.9050, 222.9216],\n",
      "        [ 78.6450,  64.4050,  82.1083,  69.0168],\n",
      "        [290.7042, 195.6609, 294.6983, 201.7324],\n",
      "        [160.4558, 146.6925, 165.0472, 153.2784],\n",
      "        [213.8866, 107.9661, 218.0859, 113.6432],\n",
      "        [292.0615, 121.4115, 295.7265, 126.4519],\n",
      "        [295.3268, 117.8782, 299.1170, 123.3251],\n",
      "        [ 74.2185,  60.8539,  77.9568,  65.0293],\n",
      "        [126.7374, 219.5645, 130.7390, 224.9159],\n",
      "        [291.8518, 197.4149, 296.0793, 203.6358],\n",
      "        [142.7804, 148.3908, 146.5451, 152.8839],\n",
      "        [161.5619, 145.0132, 166.0837, 151.5022],\n",
      "        [  2.0183, 138.1024, 252.6831, 221.9769],\n",
      "        [ 73.1403, 168.0604, 326.2929, 240.2615],\n",
      "        [ 65.5316, 135.5350,  68.5190, 139.5116],\n",
      "        [115.5978,  73.1644, 119.5232,  78.1255],\n",
      "        [121.9644, 170.0641, 287.7973, 278.1302],\n",
      "        [116.5678,  73.7460, 120.6372,  79.0178],\n",
      "        [ 76.4799,  65.1646,  79.9355,  69.6314],\n",
      "        [197.8943,  76.4799, 201.6508,  81.4340],\n",
      "        [ 65.5471, 133.2426,  68.4694, 137.2921],\n",
      "        [131.1238, 220.4232, 136.3152, 225.7767],\n",
      "        [ 74.5137,  62.2808,  78.1103,  66.3259],\n",
      "        [133.5102, 220.6831, 139.1252, 226.3639],\n",
      "        [ 85.4698, 166.2216,  91.2202, 171.9572],\n",
      "        [251.9392, 121.7432, 255.2356, 126.0717],\n",
      "        [238.9194, 123.2726, 242.5657, 127.8887],\n",
      "        [122.7728, 217.5984, 127.1045, 222.7740],\n",
      "        [289.1503, 195.2658, 293.0027, 201.1366],\n",
      "        [123.9235, 217.9983, 128.5271, 223.0962],\n",
      "        [238.9408, 121.5287, 242.7107, 126.3456],\n",
      "        [124.8709, 207.4651, 340.7697, 271.8620],\n",
      "        [250.2595, 125.3473, 254.0141, 129.1541],\n",
      "        [126.2186, 151.0048, 131.2158, 155.0172],\n",
      "        [ 74.7153,  64.6270,  78.1765,  68.9912],\n",
      "        [125.4693, 216.9313, 129.7353, 221.7263],\n",
      "        [290.9730, 121.8704, 294.6994, 126.7089],\n",
      "        [291.8636, 194.8454, 295.7793, 200.6588],\n",
      "        [199.0359,  76.8624, 202.7290,  82.1339],\n",
      "        [127.2690, 148.9119, 131.9861, 153.6108],\n",
      "        [251.6397, 125.4866, 255.1055, 129.4925],\n",
      "        [204.7590, 108.3858, 208.8615, 112.6325],\n",
      "        [108.8464, 218.4315, 112.8807, 223.1108],\n",
      "        [200.8365, 106.6665, 204.7346, 111.6271],\n",
      "        [162.7153, 146.3306, 167.2408, 152.4358],\n",
      "        [245.1568, 124.6039, 249.0942, 127.9221],\n",
      "        [196.6166,  75.8842, 200.3861,  81.1794],\n",
      "        [170.8593, 151.9518, 361.4803, 268.2283],\n",
      "        [236.9431, 124.9088, 240.5778, 129.3728],\n",
      "        [301.9933,  78.1253, 387.7115, 247.8517],\n",
      "        [ 71.7421,  60.8533,  75.1907,  65.1908],\n",
      "        [126.4129,  83.2550, 371.8116, 160.1904],\n",
      "        [140.0370, 217.6964, 144.8257, 224.3907],\n",
      "        [251.6403, 123.2675, 255.1059, 127.4043],\n",
      "        [281.3740,  61.5973, 285.2846,  67.0471],\n",
      "        [265.4202, 123.9299, 268.6785, 127.9345],\n",
      "        [143.6066, 147.6620, 147.3459, 151.9892],\n",
      "        [162.8244, 143.5535, 167.3018, 150.2285],\n",
      "        [ 78.4363,  62.7267,  81.5662,  67.0999],\n",
      "        [293.3468, 192.0135, 296.9536, 196.7914],\n",
      "        [  0.0000, 207.9550, 209.6004, 271.3207],\n",
      "        [305.3137, 116.3591, 308.9242, 121.0918],\n",
      "        [152.6298, 146.9942, 156.3271, 152.6407],\n",
      "        [ 72.9573,  64.5273,  76.7375,  68.7556],\n",
      "        [128.1160, 151.2014, 132.7193, 155.7309],\n",
      "        [ 76.2135,  61.6039,  80.0324,  65.8257],\n",
      "        [106.4955, 216.5376, 110.8492, 220.9947],\n",
      "        [122.8259, 219.3550, 127.2811, 224.9101],\n",
      "        [255.6732, 230.2395, 260.2651, 234.9806],\n",
      "        [164.2937, 144.6536, 168.3754, 150.9443],\n",
      "        [161.1242, 148.3127, 166.3488, 154.4098],\n",
      "        [293.3473, 197.9464, 297.8518, 204.0792],\n",
      "        [250.3327, 121.8625, 253.8497, 126.0766],\n",
      "        [292.3349, 119.1016, 295.9544, 124.1716],\n",
      "        [291.8126, 192.5586, 295.4855, 197.8201],\n",
      "        [ 72.2403,  62.5021,  76.2454,  66.2124],\n",
      "        [ 71.3236,  66.0685,  75.6907,  70.2729],\n",
      "        [295.8606,  60.7590, 299.3749,  64.3440],\n",
      "        [160.3796, 149.8846, 165.0502, 155.6300],\n",
      "        [160.7661, 202.6860, 166.4864, 208.1425],\n",
      "        [ 10.3795, 181.2976, 241.9126, 247.4243],\n",
      "        [  0.0000, 219.1031, 352.2442, 296.6576],\n",
      "        [ 64.2589, 135.7554,  67.4587, 139.5543],\n",
      "        [267.0783, 124.1603, 270.2836, 127.9847],\n",
      "        [ 85.6190, 168.0560,  91.5462, 174.5425],\n",
      "        [290.5588, 193.1555, 294.2505, 198.5322],\n",
      "        [ 78.0240,  60.3534,  81.3215,  64.9773],\n",
      "        [127.6496, 159.5440, 131.5818, 163.7594]], grad_fn=<StackBackward0>), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1]), 'scores': tensor([0.5489, 0.5311, 0.5277, 0.5257, 0.5235, 0.5235, 0.5225, 0.5196, 0.5174,\n",
      "        0.5160, 0.5134, 0.5130, 0.5128, 0.5108, 0.5107, 0.5101, 0.5098, 0.5094,\n",
      "        0.5094, 0.5089, 0.5076, 0.5072, 0.5059, 0.5057, 0.5045, 0.5039, 0.5038,\n",
      "        0.5035, 0.5031, 0.5027, 0.5026, 0.5024, 0.5020, 0.5011, 0.5010, 0.5001,\n",
      "        0.5000, 0.4998, 0.4993, 0.4992, 0.4988, 0.4987, 0.4986, 0.4982, 0.4976,\n",
      "        0.4974, 0.4974, 0.4973, 0.4970, 0.4969, 0.4967, 0.4967, 0.4961, 0.4959,\n",
      "        0.4959, 0.4956, 0.4955, 0.4952, 0.4950, 0.4950, 0.4948, 0.4939, 0.4935,\n",
      "        0.4934, 0.4931, 0.4927, 0.4921, 0.4917, 0.4917, 0.4913, 0.4912, 0.4910,\n",
      "        0.4906, 0.4901, 0.4899, 0.4898, 0.4896, 0.4893, 0.4893, 0.4893, 0.4891,\n",
      "        0.4889, 0.4886, 0.4879, 0.4876, 0.4875, 0.4874, 0.4874, 0.4868, 0.4868,\n",
      "        0.4867, 0.4867, 0.4864, 0.4864, 0.4862, 0.4857, 0.4856, 0.4856, 0.4855,\n",
      "        0.4853], grad_fn=<IndexBackward0>)}, {'boxes': tensor([[1.1904e+01, 1.2997e+02, 1.0776e+02, 4.0120e+02],\n",
      "        [1.1330e-01, 1.6939e+02, 2.9596e+01, 4.1897e+02],\n",
      "        [4.5342e+00, 2.4322e+02, 1.0662e+02, 4.8877e+02],\n",
      "        [4.1483e-01, 4.6611e+01, 3.0951e+01, 3.1274e+02],\n",
      "        [2.6390e+01, 1.0735e+02, 1.3363e+02, 3.2167e+02],\n",
      "        [4.0370e+00, 1.6709e+02, 1.9704e+02, 3.5230e+02],\n",
      "        [3.2347e+01, 2.3678e+02, 1.4768e+02, 4.1601e+02],\n",
      "        [8.4726e+01, 1.4733e+02, 2.1735e+02, 3.5721e+02],\n",
      "        [8.9279e+01, 1.9426e+02, 2.3593e+02, 4.3288e+02],\n",
      "        [0.0000e+00, 2.6941e+02, 2.0821e+02, 4.4947e+02],\n",
      "        [1.4453e+02, 7.3133e+01, 1.4985e+02, 7.9610e+01],\n",
      "        [2.9985e+02, 1.4396e+02, 3.0485e+02, 1.4944e+02],\n",
      "        [1.9290e+02, 1.8012e+02, 3.2607e+02, 3.9052e+02],\n",
      "        [0.0000e+00, 1.7061e+02, 5.1317e+01, 4.5780e+02],\n",
      "        [0.0000e+00, 0.0000e+00, 7.0880e+01, 4.5560e+02],\n",
      "        [3.0159e+02, 1.4320e+02, 3.0681e+02, 1.4870e+02],\n",
      "        [1.1421e+02, 1.0230e+02, 2.4231e+02, 3.0327e+02],\n",
      "        [1.6237e+02, 1.0802e+02, 2.9320e+02, 3.0349e+02],\n",
      "        [7.6738e+00, 2.0885e+02, 2.6236e+02, 3.9114e+02],\n",
      "        [1.2905e+02, 2.5344e+02, 2.5596e+02, 4.4272e+02],\n",
      "        [1.7910e+02, 3.8681e+02, 1.8388e+02, 3.9255e+02],\n",
      "        [2.6541e+02, 7.9273e+01, 2.7069e+02, 8.4750e+01],\n",
      "        [1.2718e+02, 1.6266e+02, 1.3274e+02, 1.6834e+02],\n",
      "        [5.4486e+00, 1.1944e+02, 2.7702e+02, 2.3696e+02],\n",
      "        [0.0000e+00, 3.4911e+02, 2.6583e+02, 4.4593e+02],\n",
      "        [6.2904e+01, 1.7805e+02, 3.2470e+02, 4.8311e+02],\n",
      "        [2.9996e+02, 1.4621e+02, 3.0487e+02, 1.5167e+02],\n",
      "        [2.7427e+02, 1.3684e+02, 3.8736e+02, 3.6896e+02],\n",
      "        [3.1693e+02, 1.4747e+02, 3.2206e+02, 1.5404e+02],\n",
      "        [1.2750e+02, 1.5831e+02, 2.6632e+02, 3.8181e+02],\n",
      "        [4.9934e+01, 2.8466e+02, 3.5351e+02, 3.8833e+02],\n",
      "        [7.0180e+01, 9.0520e+01, 1.9443e+02, 2.8994e+02],\n",
      "        [2.4182e+02, 1.0062e+02, 3.6820e+02, 2.8342e+02],\n",
      "        [1.7740e+02, 3.8635e+02, 1.8275e+02, 3.9189e+02],\n",
      "        [3.0333e+02, 1.4219e+02, 3.0823e+02, 1.4779e+02],\n",
      "        [2.6353e+02, 7.8311e+01, 2.6848e+02, 8.3757e+01],\n",
      "        [1.7757e+02, 3.8812e+02, 1.8286e+02, 3.9399e+02],\n",
      "        [4.7670e+01, 1.6201e+02, 3.1815e+02, 3.4211e+02],\n",
      "        [0.0000e+00, 1.4636e+02, 1.6440e+02, 4.9851e+02],\n",
      "        [3.1522e+02, 1.4833e+02, 3.2022e+02, 1.5418e+02],\n",
      "        [1.9556e+02, 5.3234e+01, 3.2681e+02, 2.5849e+02],\n",
      "        [1.3412e+02, 6.2334e+01, 2.6805e+02, 2.5752e+02],\n",
      "        [2.9050e+02, 8.0984e+01, 2.9604e+02, 8.6723e+01],\n",
      "        [1.7505e+01, 3.0492e+01, 1.0268e+02, 2.8902e+02],\n",
      "        [1.9224e+02, 2.4290e+02, 1.9666e+02, 2.4874e+02],\n",
      "        [2.7497e+02, 3.2115e+02, 2.8082e+02, 3.2741e+02],\n",
      "        [5.9447e+01, 2.0657e+02, 3.7657e+02, 3.0837e+02],\n",
      "        [3.1311e+02, 2.2329e+02, 3.1821e+02, 2.2837e+02],\n",
      "        [3.1004e+02, 2.8075e+02, 3.1547e+02, 2.8632e+02],\n",
      "        [1.7574e+02, 3.8626e+02, 1.8093e+02, 3.9169e+02],\n",
      "        [2.5285e+02, 2.3891e+02, 2.5893e+02, 2.4461e+02],\n",
      "        [1.8321e+02, 3.8819e+02, 1.8770e+02, 3.9426e+02],\n",
      "        [3.0337e+02, 1.4967e+02, 3.0871e+02, 1.5406e+02],\n",
      "        [3.0316e+02, 1.4574e+02, 3.0780e+02, 1.5125e+02],\n",
      "        [1.9365e+02, 2.4221e+02, 1.9809e+02, 2.4849e+02],\n",
      "        [3.0039e+02, 1.4828e+02, 3.0528e+02, 1.5395e+02],\n",
      "        [1.3772e+02, 1.5886e+02, 1.4350e+02, 1.6486e+02],\n",
      "        [3.1086e+02, 2.7897e+02, 3.1556e+02, 2.8393e+02],\n",
      "        [3.0562e+02, 1.4333e+02, 3.0985e+02, 1.4834e+02],\n",
      "        [2.6687e+02, 7.9797e+01, 2.7250e+02, 8.5156e+01],\n",
      "        [3.0074e+02, 3.1927e+02, 3.0595e+02, 3.2478e+02],\n",
      "        [1.9409e+02, 2.3898e+02, 1.9846e+02, 2.4491e+02],\n",
      "        [2.5249e+02, 2.4101e+02, 2.5869e+02, 2.4660e+02],\n",
      "        [3.1160e+02, 3.2048e+02, 3.1655e+02, 3.2715e+02],\n",
      "        [3.1332e+02, 1.4824e+02, 3.1838e+02, 1.5352e+02],\n",
      "        [1.9225e+02, 2.4077e+02, 1.9650e+02, 2.4653e+02],\n",
      "        [1.3773e+02, 1.6162e+02, 1.4322e+02, 1.6758e+02],\n",
      "        [3.0563e+02, 1.4539e+02, 3.0986e+02, 1.5056e+02],\n",
      "        [8.4163e+01, 1.9834e+02, 8.9226e+01, 2.0329e+02],\n",
      "        [3.7430e+01, 2.9671e+02, 3.1856e+02, 4.5693e+02],\n",
      "        [1.5275e+02, 3.0457e+02, 1.5748e+02, 3.0976e+02],\n",
      "        [2.8871e+02, 8.2637e+01, 2.9396e+02, 8.8418e+01],\n",
      "        [1.9473e+02, 2.3831e+02, 1.9956e+02, 2.4350e+02],\n",
      "        [3.1685e+02, 1.5006e+02, 3.2191e+02, 1.5657e+02],\n",
      "        [1.4322e+02, 7.6017e+01, 1.4805e+02, 8.1977e+01],\n",
      "        [7.0049e+01, 3.7420e+02, 3.5438e+02, 4.5574e+02],\n",
      "        [3.1256e+02, 2.7852e+02, 3.1669e+02, 2.8310e+02],\n",
      "        [2.9096e+02, 8.3313e+01, 2.9612e+02, 8.8971e+01],\n",
      "        [3.0130e+02, 1.5035e+02, 3.0659e+02, 1.5609e+02],\n",
      "        [2.7024e+02, 2.4054e+02, 3.8394e+02, 4.5718e+02],\n",
      "        [3.1518e+02, 1.4555e+02, 3.1984e+02, 1.5118e+02],\n",
      "        [3.0289e+02, 1.5182e+02, 3.0837e+02, 1.5609e+02],\n",
      "        [1.7682e+02, 3.8393e+02, 1.8215e+02, 3.8891e+02],\n",
      "        [2.8940e+02, 2.4407e+02, 2.9456e+02, 2.4920e+02],\n",
      "        [3.0989e+02, 1.4999e+02, 3.1525e+02, 1.5408e+02],\n",
      "        [3.1194e+02, 1.5000e+02, 3.1659e+02, 1.5506e+02],\n",
      "        [1.9373e+02, 2.3705e+02, 1.9828e+02, 2.4224e+02],\n",
      "        [3.1516e+02, 1.4264e+02, 3.1960e+02, 1.4802e+02],\n",
      "        [1.1341e+02, 2.4632e+02, 3.7921e+02, 4.1655e+02],\n",
      "        [1.3579e+02, 2.2383e+02, 1.4022e+02, 2.2862e+02],\n",
      "        [1.9479e+02, 2.4069e+02, 1.9925e+02, 2.4640e+02],\n",
      "        [1.0315e+02, 3.1207e+02, 1.0787e+02, 3.1713e+02],\n",
      "        [8.1423e+01, 3.2192e+02, 8.6706e+01, 3.2723e+02],\n",
      "        [8.3115e+01, 3.2149e+02, 8.8088e+01, 3.2676e+02],\n",
      "        [0.0000e+00, 3.8567e+02, 4.0000e+02, 4.9254e+02],\n",
      "        [3.0089e+02, 3.2366e+02, 3.0614e+02, 3.2917e+02],\n",
      "        [3.0526e+02, 1.4777e+02, 3.0962e+02, 1.5301e+02],\n",
      "        [1.9351e+02, 2.4442e+02, 1.9803e+02, 2.5033e+02],\n",
      "        [3.1007e+02, 1.5199e+02, 3.1549e+02, 1.5617e+02],\n",
      "        [3.1349e+02, 1.5072e+02, 3.1848e+02, 1.5603e+02]],\n",
      "       grad_fn=<StackBackward0>), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1]), 'scores': tensor([0.5769, 0.5705, 0.5696, 0.5668, 0.5546, 0.5475, 0.5431, 0.5405, 0.5390,\n",
      "        0.5385, 0.5368, 0.5366, 0.5355, 0.5351, 0.5342, 0.5326, 0.5301, 0.5293,\n",
      "        0.5291, 0.5265, 0.5242, 0.5232, 0.5231, 0.5223, 0.5213, 0.5194, 0.5189,\n",
      "        0.5184, 0.5181, 0.5180, 0.5177, 0.5171, 0.5156, 0.5154, 0.5144, 0.5129,\n",
      "        0.5128, 0.5127, 0.5126, 0.5122, 0.5119, 0.5117, 0.5105, 0.5100, 0.5093,\n",
      "        0.5086, 0.5084, 0.5079, 0.5075, 0.5073, 0.5067, 0.5061, 0.5052, 0.5051,\n",
      "        0.5048, 0.5045, 0.5043, 0.5043, 0.5039, 0.5037, 0.5036, 0.5035, 0.5027,\n",
      "        0.5025, 0.5023, 0.5020, 0.5014, 0.5010, 0.5009, 0.5007, 0.5003, 0.4983,\n",
      "        0.4977, 0.4977, 0.4974, 0.4971, 0.4968, 0.4965, 0.4964, 0.4962, 0.4958,\n",
      "        0.4957, 0.4950, 0.4946, 0.4946, 0.4945, 0.4944, 0.4940, 0.4940, 0.4938,\n",
      "        0.4937, 0.4937, 0.4934, 0.4932, 0.4930, 0.4927, 0.4925, 0.4925, 0.4921,\n",
      "        0.4920], grad_fn=<IndexBackward0>)}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "import utils\n",
    "import transforms as T\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = CansDataset(root, train=True)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "# For Training\n",
    "images,targets = next(iter(data_loader))\n",
    "images = list(image for image in images)\n",
    "\n",
    "\n",
    "print(images[0])\n",
    "\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "\n",
    "output = model(images,targets)   # Returns losses and detections\n",
    "# For inference\n",
    "model.eval()\n",
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "predictions = model(x)           # Returns predictions\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Do\n",
    "Fix get item så den kører med Jonas' data\\\n",
    "Få vores data til at kører med den pretrænede model\\\n",
    "Test forward pass\\\n",
    "Benyt Holgers split til at træne med og opnå fuld model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use our dataset and defined transformations\n",
    "dataset = CansDataset(root, train=True)\n",
    "dataset_test = CansDataset(root, train=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# split the dataset in train and test set\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"device:\",device)\n",
    "\n",
    "# our dataset has three classes only - background, beer and coke\n",
    "num_classes = 3\n",
    "\n",
    "# get the model using our helper function\n",
    "#model = get_instance_segmentation_model(num_classes)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9449/3082487489.py:50: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  bbox = torch.as_tensor(bbox, dtype=torch.int64)\n",
      "/tmp/ipykernel_9449/3082487489.py:50: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  bbox = torch.as_tensor(bbox, dtype=torch.int64)\n",
      "/tmp/ipykernel_9449/3082487489.py:50: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  bbox = torch.as_tensor(bbox, dtype=torch.int64)\n",
      "/tmp/ipykernel_9449/3082487489.py:50: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  bbox = torch.as_tensor(bbox, dtype=torch.int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/590]  eta: 0:16:15  lr: 0.000013  loss: 1.7305 (1.7305)  loss_classifier: 1.3977 (1.3977)  loss_box_reg: 0.3283 (0.3283)  loss_objectness: 0.0035 (0.0035)  loss_rpn_box_reg: 0.0010 (0.0010)  time: 1.6533  data: 0.2801  max mem: 2115\n",
      "Epoch: [0]  [ 10/590]  eta: 0:13:10  lr: 0.000098  loss: 1.6003 (1.5146)  loss_classifier: 1.3451 (1.2340)  loss_box_reg: 0.2644 (0.2716)  loss_objectness: 0.0049 (0.0070)  loss_rpn_box_reg: 0.0013 (0.0020)  time: 1.3628  data: 0.0304  max mem: 2379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9449/734469737.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# train for one epoch, printing every 10 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;31m# update the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MEGAsync/DTU/9. Semester/Deep Learning/object-tracking-project/02456-project/notebooks/../scripts/engine.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0mobjectness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_bbox_deltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         \u001b[0manchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mnum_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torchvision/models/detection/anchor_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image_list, feature_maps)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mimage_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         strides = [[torch.tensor(image_size[0] // g[0], dtype=torch.int64, device=device),\n\u001b[0m\u001b[1;32m    123\u001b[0m                     torch.tensor(image_size[1] // g[1], dtype=torch.int64, device=device)] for g in grid_sizes]\n\u001b[1;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_cell_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torchvision/models/detection/anchor_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mimage_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         strides = [[torch.tensor(image_size[0] // g[0], dtype=torch.int64, device=device),\n\u001b[0m\u001b[1;32m    123\u001b[0m                     torch.tensor(image_size[1] // g[1], dtype=torch.int64, device=device)] for g in grid_sizes]\n\u001b[1;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_cell_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# let's train it for 10 epochs\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    #evaluate(model, data_loader_test, device=device)  \n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, data_loader_test, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick one image from the test set\n",
    "img, _ = dataset_test[0]\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4653/1439137571.py:45: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  bbox = torch.as_tensor(bbox, dtype=torch.int64)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'tolist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4653/1534964080.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MEGAsync/DTU/9. Semester/Deep Learning/object-tracking-project/02456-project/notebooks/../scripts/engine.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, data_loader, device)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Test:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mcoco\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_coco_api_from_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0miou_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_iou_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mcoco_evaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCocoEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoco\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MEGAsync/DTU/9. Semester/Deep Learning/object-tracking-project/02456-project/notebooks/../scripts/coco_utils.py\u001b[0m in \u001b[0;36mget_coco_api_from_dataset\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCocoDetection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_coco_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MEGAsync/DTU/9. Semester/Deep Learning/object-tracking-project/02456-project/notebooks/../scripts/coco_utils.py\u001b[0m in \u001b[0;36mconvert_to_coco_api\u001b[0;34m(ds)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mareas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"area\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0miscrowd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"iscrowd\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"masks\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"masks\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'tolist'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
